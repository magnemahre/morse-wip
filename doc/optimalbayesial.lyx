#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand\thesubsection{\alph{subsection}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman bookman
\font_sans default
\font_typewriter cmtt
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Optimal Bayesian Estimation of the State of a Probabilistically Mapped Memory-Co
nditional Markov Process with Application to Manual Morse Decoding
\end_layout

\begin_layout Author
Edison Lee Bell
\end_layout

\begin_layout Date
September 1977
\end_layout

\begin_layout Abstract
This dissertation investigates the problem of automatic transcription of
 the hand-keyed Morse signal.
 A unified model for this signal process transmitted over a noisy channel
 is shown to be a system in which the state of the Morse process evolves
 as a memory-conditioned probabilistic mapping of a conditional Markov process,
 with the state of this process playing the role of a parameter vector of
 the channel model.
 The decoding problem is then posed as finding an optimal estimate of the
 state of the Morse process, given a sequence of measurements of the detected
 signal.
 The Bayesian solution to this nonlinear estimation problem is obtained
 explicitly for the parameter-conditional linear- gaussian channel, and
 the resulting optimal decoder is shown to consist of a denumerable but
 exponentially expanding set of linear Kalman filters operating on a dynamically
 evolving trellis.
 Decoder performance is obtained by computer simulation, for the case of
 random letter message texts.
 For nonrandom texts, further research is indicated to specify linguistic
 and format- dependent models consistent with the model structure developed
 herein.
 
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Acknowledgments
\end_layout

\begin_layout Standard
I wish to express my deep appreciation to Dr.
 Stephen Jauregui for his continual support and patience during the preparation
 of this dissertation.
 I am also grateful to all the members of my doctoral committee for their
 guidance and suggestions in the development and expression of the ideas
 presented in this work.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The problem of automatically transcribing the hand-keyed manual morse (HKM)
 signal with an acceptable error rate, without exact knowledge of the sender's
 keying characteristics and transmitted signal parameters, has, in general,
 remained unsolved.
 The easier companion problem of automatically transcribing a Morse signal
 sent by a keyboard (KAM) , and whose transmitted frequency is known, has
 largely been solved, and a number of Morse decoders are commercially available
 for this task.
 These decoders also can be used on the HKM signal, but with considerable
 loss in performance except in cases of very good keying quality.
 
\end_layout

\begin_layout Standard
The difficulty of automatically transcribing the HKM signal (problems in
 frequency acquisition and detection aside) is often not recognized by the
 uninitiated.
 This difficulty is analogous to that of designing an automatic speech recogniti
on device.
 While the analogy cannot be taken too far, certain parallels are evident.
 The HKM signal, being a human-generated process, has all the char- acteristics
 of individuality associated with such a process.
 No two senders of Morse send in exactly the same way, just as no two speakers
 speak in exactly the same way.
 Yet a trained Morse operator can understand what is being sent, just as
 a person who understands the language of a speaker can understand (almost)
 anyone who speaks that language, whatever the individual characteristics
 of his speech.
 A Morse transcription machine for HKM which bases its deci- sions solely
 on the local Morse symbols (dot, dash, element space, character space,
 word space, pause) can, with some imagination, be likened to a situation
 in which a person who does not know English attempts to translate a spoken
 English phrase by isolating the syllables of the words.
 Clearly the Morse transcription task is not quite so difficult as this
 analogy since there are only six "syllables" in Morse; yet the analogy
 is illustrative of the difficulty of transcribing the HKM process.
 
\end_layout

\begin_layout Standard
On the other hand, the KAM signal can be likened to a teletype signal with
 a well-defined structure.
 Thus it is sufficient to decode such a signal on the basis of the baud
 structure, since there is a one-to-one mapping from the code words to the
 text.
 This non-singular mapping accounts for the relative ease of decoding a
 demodulated KAM signal.
 
\end_layout

\begin_layout Standard
The above analogy has tacitly assumed that the Morse waveform was perfectly
 demodulated.
 In the real world of imperfect demodulation, it is clear than an HKM transcript
ion machine which uses only local information, can provide no error-correction
 capability to correct incorrectly demodulated Morse symbols.
 Thus as a result of a single incorrect demodulation decision, an entire
 letter (two letters if the symbol was a character space) is transcribed
 incorrectly.
 Demodulation, therefore, must be considered as an integral part of the
 HKM processor, and this processor must have some knowledge of the Morse
 "language" in order to provide error- correction capability.
 
\end_layout

\begin_layout Standard
This paper reports the results of an investigation into the problem of automatic
ally transcribing the HKM process.
 The problem is attacked from the point-of-view of optimal estimation and
 modern information theory.
 Theoretical results are derived which can be directly applied to the design
 of an optimal HKM transcriber.
 It is shown that such an optimal transcriber is unrealizable in the practical
 sense, but that a suboptimal transcriber which can be made arbitrarily
 close to optimal is realizable.
 Lower bounds on the theoretical error-rate performance of an ideal transcriber
 are obtained as a function of signal-to-noise ratio, keying characteristics,
 and HKM model complexity.
 The performance of the suboptimal transcriber is obtained by computer simulatio
n and compared to the theoretical results for the optimal transcriber.
 Finally, the suboptimal transcriber is tested against a limited set of
 field data in order to validate the simulations.
 
\end_layout

\begin_layout Standard
The report is organized into two parts: theoretical and application.
 In the theoretical section, a unified model structure for the HKM process
 is derived which may account for code symbol dependencies, variation in
 data rate, operator sending anomalies, source letter context, message format,
 and linguistic dependencies.
 A channel model is constructed to account for transmitter, propagation,
 and receiver effects.
 The resulting modeled system is shown to be a system in which the state
 of the HKM process evolves as a memory-conditioned probabilistic mapping
 of a conditional Markov process, with the state of this process playing
 the role of a parameter vector of the channel and measurement models.
 The joint demodulation, decoding, and translation problem is then posed
 as finding an optimal estimate of the discrete state of the HKM signal
 process, given a sequence of noisy measure- ments of the detected signal.
 The Bayesian solution to this nonlinear estimation problem is obtained
 explicitly for the case of parameter-conditional linear-gaussian channel
 and measurement models, and the resulting optimal Morse transcription machine
 is shown to consist of a denumerable but exponentially expanding set of
 linear Kalman filters operating on a trellis defined by the discrete state
 values of the parameter vector.
 Because of the exponential growth, the optimal estimator is unrealizable,
 and a realizable suboptimal solution which adaptively restricts the growth
 of the trellis is obtained.
 
\end_layout

\begin_layout Standard
The application section shows how a specific model of the HKM process results
 from the general model constructed in the theoretical section.
 It is shown in principle how the generality of the model readily provides
 for any level of complexity in modeling an actual Morse message, i.e.
 from a very simple model of local Morse symbols up to and including a complex
 model of syntactic and semantic rules for the Morse "language.
 " It is shown theoretically how context may be used to provide error-correction
 capability and reduce the lower- bound on output letter-error rate.
 Simulation results are obtained which confirm the expected improved performance
 for increasingly complex modeling of the Morse message.
 
\end_layout

\begin_layout Section
Problem description
\end_layout

\begin_layout Standard
The statement of the problem is actually very simple: Obtain a processor
 which will transcribe hand-keyed manual Morse as well as a human operator.
 The simplicity of the statement, however, belies the complexity of describing
 a "hand-keyed manual Morse" signal and the difficulty of quantifying the
 phrase "as well as a human operator." 
\end_layout

\begin_layout Subsection
THE HAND-KEYED MANUAL MORSE (HKM) SIGNAL PROCESS 
\end_layout

\begin_layout Standard
As used throughout this report, the term 
\bar under
HKM signal
\bar default
 refers to International Morse Code and its derivatives sent manually by
 key, mechanical bug, or electronic bug.
 The 
\bar under
baseband
\bar default
 HKM process is the output voltage level of the keyer and is represented
 by the logic levels and 1, corresponding to the states "key up" and "key
 down." The six 
\bar under
symbols
\bar default
 of the code are: 
\bar under
dot
\bar default
 , 
\bar under
dash
\bar default
 , 
\bar under
element-space
\bar default
 , 
\bar under
character-space
\bar default
 , 
\bar under
word-space
\bar default
 , and 
\bar under
pause
\bar default
 .
 The term 
\bar under
element
\bar default
 (or 
\bar under
baud
\bar default
 ) refers to the standard time unit of the code; its actual duration in
 seconds will of course vary with sending speed.
 Standard Morse code consists of the symbol durations shown in Table I.
 
\end_layout

\begin_layout Standard
The standard word (including word-space) in Morse communication is 50 elements
 in length.
 Thus the standard element duration in seconds for a given sending speed
 is 6/5 times the reciprocal of the speed in words-per-minute .
 The instantaneous data rate for an HKM signal is defined to be 6/5 times
 the reciprocal of the duration of the symbol (in seconds) divided by the
 standard duration in elements; e.g., the 
\bar under
instantaneous data rate
\bar default
 for a dash of duration 60 msec is (6/5) / (1/.
 020) = 60 wpm.
 
\end_layout

\begin_layout Standard
An HKM signal differs from the standard Morse signal in that the instantaneous
 data rate is a random variable, resulting in symbol durations which are
 random.
 The element duration is defined to be the mean value of the dot duration;
 this mean value is also a random variable.
 The HKM signal may exhibit a large variation in both element duration and
 instantaneous data rate.
 The modeling of these random variables is discussed in section VI.
 A.
 The distributions of element duration and instantaneous data rate are unique
 to a particu- lar sending operator, and in most cases depend on the type
 of traffic being sent, and on the intended recipient of the signal as well.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption

\begin_layout Plain Layout
Standard Morse Symbols
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Symbol
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Duration (in elements)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dot
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dash
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Element-space
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
^
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Character-space
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
~
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Word-space
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
W
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pause
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
THE HKM SIGNAL CHANNEL
\end_layout

\begin_layout Standard
The HKM signal process is usually transmitted at HF by a transmitter whose
 final amplifier is on-off keyed (00K) by the keyer, although in some cases,
 the oscillator itself is on-off keyed.
 Because of the effect of transients in the transmitter, the signal is usually
 chirped to some extent, the magnitude of the chirp being indicative of
 the quality of the transmitter design and state of maintenance.
 For well-designed, properly maintained transmitters, the chirp is on the
 order of tens of Hertz.
 Poorly designed or improp- erly maintained transmitters may exhibit as
 much as 3 00Hz chirp, as well as random drift of the nominal carrier fre-
 quency.
 Thus in most cases, signal detection must be accom- plished by using an
 envelope detector since the phase of the signal is not known.
 
\end_layout

\begin_layout Standard
In addition to the signal uncertainties caused by the transmitter itself,
 the signal is also corrupted by both additive and multiplicative noise
 in the form of atmospherics, interference, and fading, which at HF is nonstatio
nary .
 Thus demodulation of the 00K Signal must be accomplished in the face of
 frequency, phase, and amplitude uncertainty, along with incomplete knowledge
 of the noise statistics.
 
\end_layout

\begin_layout Subsection
OPERATOR PERFORMANCE
\end_layout

\begin_layout Standard
The ultimate goal of the Morse transcriber is to provide output copy with
 an error rate approaching that which a typical human operator provides.
 The human operator rapidly adapts to changing signal and channel parameters
 and can provide reliable copy of a highly variable HKM signal in the presence
 of numerous other Morse and non-Morse signals.
 The operator is obviously aided by an understanding of the context of the
 message, the format, and the Morse "language." 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
(see original document for this figure)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
(see original document for this figure)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RATE (wpm)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Factor (dB)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-5.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-3.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-2.3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
15
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1.8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
16
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1.4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-0.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.6
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Operator performance adjustment factor for sending speeds (from Lane[3])
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The available data on operator performance is summarized in Figures 1 and
 2.
 Figure 1 is a plot of error rate vs.
 SNR for an actual communications link in the LF band reported by Watt et.
 al.
 [1] , while Figure 2 shows the performance obtained in a laboratory experiment
 [2] .
 Both tests were conducted using random five-letter code groups as the test
 message.
 Table II, from Lane [3] , shows the number of dB which must be added or
 subtracted from the abscissa of the performance curve to obtain the performance
 for different speeds of transmission.
 Clearly the laboratory tests show a better performance capability for the
 human operator than that obtained for the actual communication link, with
 a difference of about 2-3 dB for equal error rates.
 Such an observation indicates that one must design the automated transcriber
 using the laboratory performance measurements in order to obtain the required
 performance under field conditions for the same SNR.
 
\end_layout

\begin_layout Standard
The error rates discussed above were obtained using a text consisting of
 independent letters (5-letter code groups) .
 For a text which has more structure than random letters, whether through
 linguistic content, known message format, or increased semantic content,
 the human operator will take advantage of the structure to effectively
 reduce his average error rate.
 His error rate, however, for those portions of a message which exhibit
 uncertainty equivalent to independent letters, will remain at that for
 independent letters.
 Thus although his error rate for those portions of a message which have
 a high information content will not decrease, the transcribed message will
 be much more "readable," and the more costly errors will be much easier
 to locate in his output copy.
 As an example of "readability", consider the two messages shown below,
 each with a 10% error rate, including spacing errors.
 The first message is of low information content and is readable, although
 with some difficulty; the second is a message with higher information content.
 (These two messages were generated by using a random number generator to
 obtain the errors, which may not correspond to typical morse substituions.
 )
\end_layout

\begin_layout Description
Message
\begin_inset space ~
\end_inset

1: 
\end_layout

\begin_layout LyX-Code
THIS IS AN RX A9P LE OF EN G LI SH TE XT  
\end_layout

\begin_layout LyX-Code
WITH AN ERROR RATE OF 10 PERCENK.
 THC  
\end_layout

\begin_layout LyX-Code
ERRORS INCLUDE SPA CING BETWEEN LE TTERS  
\end_layout

\begin_layout LyX-Code
AS WELL AS THE WP1D SPACE.
 MS CAN3 E  
\end_layout

\begin_layout LyX-Code
SEEN, THIS TEXT IS ON TH E THRESHOLDO F  
\end_layout

\begin_layout LyX-Code
ACC EPTABILRTY AN D REQUIRA 2 SlAE  
\end_layout

\begin_layout LyX-Code
DIFW8C U LTX TO R EAD.
\end_layout

\begin_layout Description
Message
\begin_inset space ~
\end_inset

2:
\end_layout

\begin_layout LyX-Code
BM GEZRGE P BURDELL TO JOXN BUUYEL  
\end_layout

\begin_layout LyX-Code
L12 3 EASW S T BEW YORK BT  
\end_layout

\begin_layout LyX-Code
PSE C ALL NAMP HO NE NO 555 1233 AND  
\end_layout

\begin_layout LyX-Code
TELL SIM WILL NOW DRR IVE KENNE DY  
\end_layout

\begin_layout LyX-Code
AVTAN 17 38 12 JU LFLT NO 63  WILL 
\end_layout

\begin_layout LyX-Code
DEPANT FOX WAMH AT 231 9 12 JUL.
  
\end_layout

\begin_layout Standard
The obvious point of this exercise is that average letter error rate alone
 is not a definitive measure by which the efficiency of a transcriber (either
 human or machine) can be judged, except for messages consisting of random
 letters.
 Secondly, it is clear that an automatic transcriber which does not use
 the message context and structure (linguistics, semantics, format) to decode
 the received message will not be capable of producing a transcript as readable
 as the human operator except for random letter texts.
 
\end_layout

\begin_layout Section
LOWER BOUNDS ON ERROR RATE 
\end_layout

\begin_layout Standard
In this section, information theoretic concepts are applied to the problem
 of decoding and translation of the Morse signal.
 Lower bounds on the performance of a trans- cription machine are obtained
 as a function of signal-to- noise ratio, keying quality, and decoder complexity.
 A channel model appropriate for studying the performance in this context
 is derived and its capacity determined.
 Source code models for the Morse code are also obtained, and together with
 the channel model, are used to derive a lower bound on decoded letter error
 rate.
 Although the average letter error rate, as argued in the previous section,
 is not a sufficient criterion for measuring the utility of a trans- cription
 machine in specific cases, it nevertheless provides a great deal of insight
 into the problem of determining how complex a decoder must be in order
 to approach the performance of a human operator.
 In order to obtain some intuitive appreciation of the Morse code as a source
 code, estimates of the entropy of a Morse-coded source are first determined
 under various assumptions about the source and the code.
 
\end_layout

\begin_layout Subsection
ESTIMATION OF MORSE-CODE ENTROPY 
\end_layout

\begin_layout Standard
The source entropy for a symbol -by- symbol decoder is obtained by considering
 the source to be an ensemble of Morse symbols each sent independently with
 probability equal to the expected relative frequency of occurrence of that
 symbol.
 A decoder which is designed according to a model of the source as a Markov
 chain results in a source entropy calculated on the basis of that same
 Markov model.
 Thus various levels of model complexity result in corresponding levels
 of source entropy, as seen by the decoder.
 For independent symbol sequences the source entropy for an alphabet of
 size M is given by [4]: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H=-\sum_{i=1}^{M}p(i)\log(p(i))
\]

\end_inset

p(i) = relative frequency of occurrence of symbol i 
\end_layout

\begin_layout Standard
For Markov sources the entropy is given by [4, p.
 68] 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(u)=-\sum_{i=1}^{J}q(i)H(u|s=i)
\]

\end_inset


\end_layout

\begin_layout Standard
where q(i) = limiting probability of the state s = i; 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(u/s=1)=-\sum_{k=1}^{K}P_{j}(a_{k})\log(P_{j}(a_{k}))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P_{j}(a_{k})=Pr\left[u_{l}=a_{k}|s_{l)j}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
i.e.
 the probability that source letter a, is produced when the Markov process
 is in state j at time I.
 
\end_layout

\begin_layout Subsubsection
Independent Symbols 
\end_layout

\begin_layout Standard
Consider first the case of a source modeled by independent occurrences of
 the Morse symbols.
 In this case the entropy is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H=-P_{dot}\log P_{dot}-P_{dash}logP_{dash}-P_{esp}-\log P_{esp}-P_{csp}\log P_{csp}
\]

\end_inset


\end_layout

\begin_layout Standard
The relative frequencies of the symbols in random Morse are: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P_{dot}=.26,P_{dash}=.24,P_{esp}=.36,P_{csp}=.14
\]

\end_inset


\end_layout

\begin_layout Standard
and the entropy is: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H=.26\log(.26)-.24\log(.24)-.36\log(.36)-.14log(.14)
\]

\end_inset


\end_layout

\begin_layout Standard
= 1.927 bits/Morse symbol 
\end_layout

\begin_layout Standard
Since there are 1.76 bauds per Morse symbol, on the average , the entropy
 in bits per channel digit is 
\begin_inset Formula $H=1.927/1.76=1.09bits$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection
First-Order Markov Process on a Symbol Basis 
\end_layout

\begin_layout Standard
The independent symbol model of Morse is actually only of passing interest
 since even the crudest of Morse models recognizes the fact that in Morse
 code a mark symbol (dot or dash) must always be followed by a space symbol
 (esp or csp), and vice versa.
 
\end_layout

\begin_layout Standard
A first-order Markov model has the following approximate transistion matrix
 and limiting probabilities 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{array}{c}
\\
dot\\
dash\\
esp\\
csp
\end{array}\begin{bmatrix}dot & dash & esp & csp & q(1)\\
0 & 0 & .7 & .3 & .26\\
0 & 0 & .7 & .3 & .24\\
.55 & .45 & 0 & 0 & .36\\
.5 & .5 & 0 & 0 & .14
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
Using the formulas given above for finding the entropy of a Markov source,
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(u|s=l)=-.7log(.7)-.31log(.3)=.8813
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(u|s=2)=-.7log(.7)-.3log(.3)=.8813
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(u|s=3)=.55log(.55)-.45log(.45)=.9929
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(u|s=4)=-.5log(.5)-.5log(.5)=1.0
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(u)=(.26)(.8813)+(.24)(.8813)+(.36)(.9929)+(.14)(1.0)=.938bits/Morse\, symbol=.533bits/channel\, digit
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Second-Order Markov Process On A Symbol Basis 
\end_layout

\begin_layout Standard
A second-order Markov process of the Morse Code has the approximate transition
 Matrix and limiting state probabilities as follows: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{array}{c}
.\wedge\\
.\sim\\
-\wedge\\
-\sim\\
\wedge.\\
\sim.\\
\wedge-\\
\sim.\\
\wedge-
\end{array}\begin{bmatrix}.\wedge & .\sim & -\wedge & -\sim & \wedge. & \sim. & \wedge- & \sim-\\
0 & 0 & 0 & 0 & .55 & 0 & .45 & 0\\
0 & 0 & 0 & 0 & 0 & .5 & 0 & .5\\
0 & 0 & 0 & 0 & .55 & 0 & .45 & 0\\
0 & 0 & 0 & 0 & 0 & .5 & 0 & .5\\
.7 & .3 & 0 & 0 & 0 & 0 & 0 & 0\\
.97 & .03 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & .6 & .4 & 0 & 0 & 0 & 0\\
0 & 0 & .97 & .03 & 0 & 0 & 0 & 0
\end{bmatrix}\begin{array}{c}
q(1)\\
.187\\
.073\\
.173\\
.067\\
.187\\
.073\\
.173\\
.067
\end{array}
\]

\end_inset


\end_layout

\begin_layout Standard
Again, using the formulas for the entropy of a Markov source, the entropy
 of the source for this model is found to be 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H=.858bits/Morse\, symbol=.488bits/channel\, digit
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Independent Letters 
\end_layout

\begin_layout Standard
The entropy of a source which produces equally likely independent letters
 from an alphabet of size 36 (26 alphabet letters, 10 numerals) is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H=-log(.02776)=5.17bits/ltr
\]

\end_inset


\end_layout

\begin_layout Standard
The average number of Morse symbols per letter is 7.27, resulting in an average
 entropy for the Morse symbols: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{avg}=5.17/7.27=.711bits/Morse\, symbol=.404bits/channel\, digit
\]

\end_inset


\end_layout

\begin_layout Subsubsection
English Text [5] 
\end_layout

\begin_layout Standard
For a model of an English text source, producing equally independent letters,
 the entropy is 4.76 bits/letter.
 Using the proper relative frequencies for the occurrence of each letter,
 the entropy is reduced to 4.03.
 A first- order model of English has entropy 3.32, and a second order model
 reduces the entropy to 3.1.
 A model which produces equally likely words of text has an entropy of 2.14.
 Thus if a decoder which properly uses context, linguistics, and message
 structure can be designed, then the entropy of the Morse symbol for English
 text can be as low as 2.14/7.27 = .294 bits/symbol = .
 167 bits/channel digit 
\end_layout

\begin_layout Standard
In summary, then, it can be seen that there is considerable merit in using
 for design purposes a model of the encoded source based on independent
 or Markov letters, rather than a model based on a probabilistic description
 of a sequence of Morse symbols.
 (The various entropies are tabulated in Table III.) Given an optimal demodulator
, a decoder which fully exploits the letter structure of the encoded source,
 then, can be expected to perform as well as the human operator for a source
 of independent letters.
 As discussed previously, however, any Morse message of significant interest
 does not consist of independent letters, and the human operator easily
 exploits the decrease in 
\end_layout

\begin_layout Standard
29 
\end_layout

\begin_layout Standard
TABLE III 
\end_layout

\begin_layout Standard
ENTROPY OF MORSE CODE SYMBOLS AND CHANNEL BITS 
\end_layout

\begin_layout Standard
MODEL MORSE SYMBOL CHANNEL BIT 
\end_layout

\begin_layout Standard
INDEP SYMBOLS 1.927 1.09 
\end_layout

\begin_layout Standard
FIRST-ORDER .938 .533 
\end_layout

\begin_layout Standard
MARKOV SYMBOLS 
\end_layout

\begin_layout Standard
SECOND-ORDER .858 .488 
\end_layout

\begin_layout Standard
MARKOV SYMBOLS 
\end_layout

\begin_layout Standard
INDEP SOURCE .711 .404 
\end_layout

\begin_layout Standard
LTRS 
\end_layout

\begin_layout Standard
ENGLISH TEXT .655 .372 
\end_layout

\begin_layout Standard
EQUI-PROB LTRS 
\end_layout

\begin_layout Standard
ENGLISH TEXT .457 .260 
\end_layout

\begin_layout Standard
FIRST-ORDER 
\end_layout

\begin_layout Standard
MARKOV LTRS 
\end_layout

\begin_layout Standard
ENGLISH TEXT .294 .167 
\end_layout

\begin_layout Standard
EQUI-PROB 
\end_layout

\begin_layout Standard
WORDS 
\end_layout

\begin_layout Standard
source entropy by knowing the context, linguistics, semantics, and format
 of the message.
 Conversely, any decoder which does not exploit this decrease in source
 entropy can never match the capability of the human operator, although
 it may perform well enough in some cases to be of value.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Entropy of Morse Code Symbols and Channel Bits
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Morse Symbol
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Channel Bit
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Indep Sybols
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.927
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.09
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
First-Order Markov Symbols
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.938
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.533
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Second-Order Markov Symbols
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.858
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.488
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Indep Source Ltrs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.711
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.404
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
English Text Equi-prob Ltrs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.655
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.372
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
English Text First-Order Markov Ltrs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.457
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.260
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
English Text Equi-prob Words
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.294
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.167
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
IDEALIZED HKM CHANNEL MODEL 
\end_layout

\begin_layout Standard
Since the objective here is to obtain lower bounds on error rate, and not
 an estimate of actual performance, it is appropriate to consider an idealizatio
n of the HKM process, the detection process, and optimum demodulation in
 the presence of white gaussian noise.
 As such, the output of the detector would be input to a matched filter
 whose integration time is equal to the element duration of the Morse code
 being received.
 Exact knowledge of the baud length is assumed in order that the matched
 filter can remain in synchronism with the incoming signal.
 Obviously no decoder for HKM can ever have such information with certainty,
 thus this idealization represents the best possible demodulator which can
 never be achieved in practice.
 Secondly, the error crossover probabilities (dot vs.
 dash; element-space vs.
 character space) are idealized to be discrete probabilities rather than
 considering duration densities for these symbols; the word-space is included
 as a source letter and the pause symbol is ignored for this analysis.
 Under these simplifying assumptions, the channel can be modeled as a discrete
 symmetric channel, as shown in Figure 3.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Idealized HKM Channel Model
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this model, the crossover probability 6 is related to the Morse symbol
 crossover probability by defining 6 to be the probability which yields
 the same average letter error rate as the symbol crossover probability
 on the basis of an average encoded letter.
 Since the average letter of Morse code consists of 7 symbols and 12 channel
 bits, 6 is defined by the relationship 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{s}\text{≜\left(1-\delta\right)\textasciicircum12 =\left(1-P_{es}\right)\textasciicircum7 }
\]

\end_inset


\end_layout

\begin_layout Standard
where E is the average sending letter error rate and P is the corresponding
 symbol error crossover probability.
 It will be convenient to make the following definitions on the keying quality
 of a HKM signal: 
\end_layout

\begin_layout LyX-Code
GOOD: E = .01 (P = .00143, 6 = .000837)
\end_layout

\begin_layout LyX-Code
FAIR: E = .1 (P .0149, 6 = .00874 )
\end_layout

\begin_layout LyX-Code
POOR: E = .25 (P = .0403, 5 = .0237) 
\end_layout

\begin_layout Standard
that is, a good sending operator sends the Morse symbols such that the resulting
 code stream consists of encoded letters in which 1% contain at least one
 incorrect Morse symbol; a fair operator sends with a 10% error rate; and
 a poor operator sends with a 25% error rate.
 
\end_layout

\begin_layout Standard
The crossover probability 
\begin_inset Formula $\epsilon$
\end_inset

 is just 
\begin_inset Formula $1-P_{d}$
\end_inset

, where
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $P_{d}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is the probability that the matched-filter demodulator announces the correct
 mark/space decision.
 This probability is obtained as a function of SNR by computing 
\begin_inset Formula $\frac{{E_{b}}}{N_{0}}$
\end_inset

, where 
\begin_inset Formula $E_{b}$
\end_inset

 = signal energy during an element duration and 
\begin_inset Formula $N_{0}$
\end_inset

 = one-sided noise spectral density.
 The error probability 
\begin_inset Formula $\epsilon$
\end_inset

 is then obtained from the performance curve for the probability of error
 using either coherent or envelope detection, as appropriate, followed by
 a matched filter [6] .
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Equivalent HKM BSC
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The channel shown in Figure 3 may be converted to the equivalent binary
 symmetric channel shown in Figure 4 by defining the equivalent crossover
 probability , 
\begin_inset Formula $\epsilon_{eq}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\epsilon_{eq≜p\left(1/0\right)\text{≡p\left(0/1\right)=\epsilon+\delta-2\delta\epsilon}}
\]

\end_inset


\end_layout

\begin_layout Standard
Clearly if 
\begin_inset Formula $\delta$
\end_inset

 = (perfect keying), then 
\begin_inset Formula $\epsilon_{eq}=\epsilon$
\end_inset

 , and if 
\begin_inset Formula $\epsilon_{0}=$
\end_inset

 (perfect demodulation), then 
\begin_inset Formula $\epsilon_{eq=\delta}$
\end_inset

.
\end_layout

\begin_layout Standard
Since this channel is symmetric, capacity is achieved by assigning equiprobable
 input binary symbols, and is given by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C=1+\epsilon_{eq}\log\epsilon_{eq}+\left(1-\epsilon_{eq}\right)\log\left(1-\epsilon_{eq}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Table IV gives the channel capacity as a function of signal speed and SNR
 for the KAM signal using envelope detection.
 
\end_layout

\begin_layout Subsection
CALCULATION OF LOWER BOUNDS FOR LETTER-ERROR PROBABILITY 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
HKM Channel Capacity as Function of Speed and SNR 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A lower bound average letter error rate is easily obtained by using the
 Straight-line Bound for a binary symmetric channel [4, p.
 163].
 To use this bound, it is necessary to know the number of codewords in the
 code, and the length (in binary digits) of the codewords.
 Additionally this bound only applies to stationary block codes, requiring
 construction of an equivalent stationary block code for Morse, which in
 reality is a code which produces variable length word sequences.
 Given an equivalent block code the appropriate relationship for the probability
 of codeword error, 
\begin_inset Formula $P_{e}$
\end_inset

 , is given by: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
<insert\, ugly\, formula\, here>
\]

\end_inset


\end_layout

\begin_layout Standard
This result for 
\begin_inset Formula $P_{e}$
\end_inset

is for a block code with M codewords, each of length N bits transmitted
 over a BSC with error probability 
\begin_inset Formula $\epsilon_{eq}$
\end_inset

 .
 The problem then is to construct a block code which is equivalent, in some
 sense, to the variable- length-codeword Morse code, then to determine the
 number of codewords and the length of the codewords for this equiva- lent
 code.
 Clearly the complexity of this equivalent block code will depend on how
 one chooses to model the human Morse- encoding process for the design of
 the decoder, i.e., encoding symbol-by-symbol; symbol pairs, triplets, etc.,
 letter-by- letter, letter pairs, 3-letter words, 5-letter words, etc.
 Additionally the codewords must be chosen so that the resulting encoded
 sequences are stationary in order to state that the statistical expectation
 represented by 
\begin_inset Formula $P_{e}$
\end_inset

 is the same as the expected letter error rate (expectation over time) .
 This stationarity can be ensured by requiring the encoded sequence to begin
 at a random point within a source letter [7] .
 Such a requirement is equivalent to stating that the decoder is not synchronize
d with the encoder on a letter basis; that is, the decoder has no a-priori
 knowledge of the beginning and ending of a letter of the variable-length
 word sequence produced by the Morse code.
 
\end_layout

\begin_layout Standard
Consider first the construction of an equivalent block code for Morse which
 is assumed to be encoded as a symbol pair.
 Table V shows the variable-length Morse codewords for this code.
 An equivalent set of equal length block codewords, on the basis of equal
 average codeword length, is shown in Table VI.
 It is to be noted that some code- words cannot follow other codewords in
 an encoded sequence.
 For example, the sequence 101011 cannot be followed by any codeword except
 those beginning with 10 since the sequence 11 and the sequence 1111 are
 not allowable Morse sequences .
 
\end_layout

\begin_layout Standard
In principle, the same procedure can be followed to obtain the set of codewords
 for any desired codeword length.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Variable-Length Codewords For Symbol Pairs
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Morse Symbol
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Channel Code
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.^
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-^
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1110
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.~
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-~
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
111000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
^.
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
01
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
^-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0111
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
~.
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0001
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
~-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
000111
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
Average No.
 of Channel Bits Per Morse Codeword: 4 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Equivalent Four-Bit Channel Mode For Symbol Pairs 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0001
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1010
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0010
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1011
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0011
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1101
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0101
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1110
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0111
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
No.
 of Codewords: 13
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
For sequence lengths greater than about 12, however, the sheer number of
 possibilities makes this procedure intractable.
 For obtaining codeword sets for an encoder which encodes combinations of
 more than one source letter at a time, then, another procedure is used.
 Although this procedure does not obtain all the codewords in the equiva-
 lent block code set, it obtains almost all of them and thus represents
 a lower bound on the actual number of codewords.
 
\end_layout

\begin_layout Standard
The average Morse code sequence is 7.27 symbols in length.
 For a Morse code, however, the sequence length in Morse symbols must be
 an even number (it must begin with a mark and end with a character space)
 .
 By choosing an average of 8 symbols/character for the equivalent block
 code, and by requiring that the 8th symbol be a character- space, then,
 it can be seen that it is impossible to produce a sequence of a Morse symbols
 which does not represent some character.
 It is also obvious that not all characters are represented by this code.
 Now, of the four symbols, only two are allowed in any one position of the
 sequence (since space follows mark invariably and vice versa) thus the
 possible number of synchronous Morse sequences on this basis is 
\begin_inset Formula $2\text{⁷=128}$
\end_inset

, and the minimum length of the codewords in binary digits is 
\begin_inset Formula $8*1.76=14$
\end_inset

.
 To obtain the full set of nonsynchronous codewords, each codeword is shifted
 one bit at a time and a one or zero appended, if allowable, until no new
 codewords are produced.
 To illustrate, consider the synchronous codeword 10111011101000.
 By right shifting and appending a zero and one respectively, the two additional
 codewords 01011101110100 and 11011101110100 are obtained.
 On the next shift, note that the sequence 0110 is not legal, so only three
 additional codewords are obtained: 1010..., 0010..., and 1110....
 In general, those codewords beginning with a dot (10) produce eleven additional
 codewords, and the codewords beginning with a dash (1110) produce eight
 additional codewords.
 If M = number of synchronous code- words, then M /2.
 = no.
 of codewords beginning with a dot (dash) , so the total number of nonsynchronou
s codewords is given by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M=19M_{s}/2+M_{S}=10.5M_{S}
\]

\end_inset


\end_layout

\begin_layout Standard
Table VII gives the number of binary codewords (M) and the codeword length
 (N) for the encoding procedure of interest.
 For N <_ 12, M and N are exact, as computed by the first procedure discussed
 above.
 For N > 12, M and N are lower bounds obtained by the second procedure.
 Using these values of M and N, the lower bound on 
\begin_inset Formula $P_{e}$
\end_inset

 as a function of 
\begin_inset Formula $\epsilon_{eq}$
\end_inset

is obtained.
 This value for 
\begin_inset Formula $P_{e}$
\end_inset

 is the error rate over a code of M codewords, and for the case of single
 character encoding, is the same as the average letter error rate.
 For other cases of source alphabet models, however, 
\begin_inset Formula $P_{e}$
\end_inset

 does not represent the letter error rate, since letters consist of more
 or fewer than one codeword depending on the length of the codeword.
 To determine the letter error rate, 
\begin_inset Formula $E_{l}$
\end_inset

, consider the following arguments.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Equivalent Block Codeword Set Size And Length For Morse Code
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "Case-1:"

\end_inset

Letters consisting of two or more codewords.
 
\end_layout

\begin_layout Standard
For this case, the distribution of codeword error events per letter is binomial
 with parameter 
\begin_inset Formula $P_{e}$
\end_inset

 .
 Let m be the number of codewords per letter.
 Then the probability of exactly k error events per letter is given by 
\begin_inset Formula $\binom{m}{k}P_{e}^{k}\left(1-P_{e)}\right)^{m-k}$
\end_inset

 and the probability of at least one error event per letter (i.e.
 the probability of a letter error) is given by 
\begin_inset Formula $E_{l}=1-(1-P_{e})^{m}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "Case-2:"

\end_inset

 Codewords consisting of n letters.
 
\end_layout

\begin_layout Standard
In this case, E p is lower bounded by assuming that a codeword error event
 causes a single letter error within the codeword; then 
\begin_inset Formula $E_{l}=P_{e/n}$
\end_inset

.
\end_layout

\begin_layout Standard
Figures 5-7 show plots of the lower bound on average letter error rate,
 
\begin_inset Formula $E_{l}$
\end_inset

, as a function of SNR and keying quality for several levels of assumption
 about the Morse encoding process.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Lower Bounds on Letter Error Rate for Morse Code - KAM Signal, Coherent
 Detection
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Lower Bounds on Letter Error Rate for Morse Code - KAM Signal, Envelope
 Detection
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Lower Bounds on Letter Error Rate for Hand-Keyed Morse Code - Envelope Detection
, Random Letter Source
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
A GENERAL MODEL FOR THE HKM SIGNAL PROCESS 
\end_layout

\begin_layout Standard
In this section, a general model structure which accounts for message context,
 sender operator errors, variation in date rate, and variability of element
 duration is constructed Further it is shown that various special cases
 of this model result in processes for which optimum estimation algorithms
 and decoders have been treated in the literature, some from the point of
 view of optimal estimation theory and others from an information theoretic
 viewpoint.
 
\end_layout

\begin_layout Standard
Fundamentally the model that is constructed is a sliding block coder (SBC)
 with infinite memory.
 However, instead of encoding the letters of the text into the Morse symbols
 either noiselessly or with a fidelity criterion, the encoding process is
 considered as a probabilistic mapping of the output of the SBC.
 The complexity of the SBC is determined by the degree to which the Morse
 message is desired to be modeled, from the simplest case of independent
 symbols to a highly complex syntatic and semantic model.
 While specific complex models of a Morse message are not developed in this
 investigation, the structure for imple- mentation of such models is provided
 by the general model.
 Thus the structure proposed represents a unified approach to modeling the
 Morse message from the simplest case to the most complex.
 
\end_layout

\begin_layout Subsection
A.
 BASEBAND HKM SIGNAL PROCESS 
\end_layout

\begin_layout Standard
The desired representation of the discrete-time baseband HKM process is
 a sequence of l's and O's who^e pattern of occurrence closely resembles
 that of a human operator sending a Morse text.
 By considering intuitively how a sending operator may encode the letters
 of the text, the random variables which influence the human encoding procedure
 can be recognized.
 Figure 8 is useful for visualizing this process.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Morse Encoding Process
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
At some time k, one or more letters of the text, I.
 k are encoded into a sequence of code words a, , consisting of the Morse
 symbols.
 The human operator, however, does not always send the proper Morse sequence
 for a given sequence of letters; typical mistakes are insertions and deletions
 of one or more symbols (particularly dots) , and substitutions of one symbol
 for another (particularly word-spaces for character-spaces., and character-space
s for element-spaces).
 Additionally the speed at which he is sending may vary over a period of
 time, depending on his alertness, proficiency, fatigue and the importance
 of the traffic being sent.
 
\end_layout

\begin_layout Standard
The key converts these symbols into the 0,1 logic levels of duration consistent
 with the particular Morse symbol being sent.
 The length of time that the key is in a or 1 state, however, while determined
 principally by the Morse symbol being sent, is a random variable since
 the human operator cannot always produce repeatable, precise durations
 The variability of the durations for each symbol, again, is dependent on
 the operator's proficiency, alertness, and individual sending habits.
 Consideration of these random influences leads to the model which is now
 developed.
 
\end_layout

\begin_layout Standard
Let 
\end_layout

\begin_layout Standard
x, e {K.
 ; i = 1,2}, the set of keystates; 
\end_layout

\begin_layout Standard
K 1 
\end_layout

\begin_layout Standard
a, e {A.; i = 1,2,...
 6}, the set of code symbols; 
\end_layout

\begin_layout Standard
I, £ {L.; i = 1,2,...N}, the set of source letters 
\end_layout

\begin_layout Standard
iC 1 
\end_layout

\begin_layout Standard
Further, define the following finite state memory functions : 
\end_layout

\begin_layout Standard
(1) 8, = f q (x, , 3, _ ) , the memory associated with 
\end_layout

\begin_layout Standard
keying; 
\end_layout

\begin_layout Standard
47 
\end_layout

\begin_layout Standard
(2) a k = WW' 
\end_layout

\begin_layout Standard
the memory associated with encoding; 
\end_layout

\begin_layout Standard
(3) 
\backslash

\end_layout

\begin_layout Standard
the memory associated with the source , 
\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
3, £ (B.; i = 1,2,...}, the set of key memory states; 
\end_layout

\begin_layout Standard
a, £ {A.; i = 1,2,...}, the set of encoder memory states; 
\end_layout

\begin_layout Standard
JC J.
 
\end_layout

\begin_layout Standard
X, £ {M.
 ; i = 1,2,...}, the set of source (message 
\end_layout

\begin_layout Standard
states .
 
\end_layout

\begin_layout Standard
Then the state of the process at time k is specified by the vector: 
\end_layout

\begin_layout Standard
r ^" 
\end_layout

\begin_layout Standard
^ 
\end_layout

\begin_layout Standard
- 'WWvV^ 
\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
-k = [x k ,a k' £ k ] ' -k = [e k'°V k ] ' 
\end_layout

\begin_layout Standard
For example, if f Q counts the number of samples since the last keystate
 transition, f counts the number of symbols sent since the last letter transitio
n and f
\backslash
 records the previous letter, then a specification of the state vector gives
 the current key state, code symbol, and letter being sent, along with the
 amount of time the key has been in its current state, which symbol of the
 Morse code sequence for the letter is being sent, and the previous letter.
 
\end_layout

\begin_layout Standard
To introduce the randomness associated with sending errors and variation
 in data rate, let a random control vector be defined which selects the
 Morse code sequence for the letter being transmitted, controls the instantaneou
s data rate, and the average speed of sending: 
\end_layout

\begin_layout Standard
u, £ {U.
 ; i = 1,2,...M}, the set of control vectors 
\end_layout

\begin_layout Standard
The complete state vector is now given by 
\end_layout

\begin_layout Standard
H k 
\end_layout

\begin_layout Standard
^k 
\end_layout

\begin_layout Standard
[X k ^ 
\backslash
 H/ 
\backslash
 « k 
\backslash
^ 
\end_layout

\begin_layout Standard
The probabilistic evolution of the states of the process will be fully specified
 when the following transition probabilities are determined: 
\end_layout

\begin_layout Standard
Pr[s.
 = S .
 , u.
 = U .
 , a, = I Is.
 , = S .
 u.
 , = U , a.
 , = E^] — k 1 — k — 3 — k -m 1 — k-1 — n' — k-1 p — k-1 -q 
\end_layout

\begin_layout Standard
49 
\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
(S .
 ; i = 1, 2 , .
 .
 .
 R} is the set of all state values, 
\end_layout

\begin_layout Standard
and 
\end_layout

\begin_layout Standard
{£.; i=l,2,...Q} is the set of all memory states.
 
\end_layout

\begin_layout Standard
This state transition probability matrix is now derived in terms of the
 components of the vector s, .
 
\end_layout

\begin_layout Standard
Let the evolution of the keystate, which is dependent only on its present
 and past inputs and its past outputs be described by the transition probabiliti
es: 
\end_layout

\begin_layout Standard
(4) P(x k |a k a k _ x ^ ) A Pr[x k = K .
 | a k = A a^ = A , S k _ 1 = B fc ] 
\end_layout

\begin_layout Standard
Similarly the evolution of the encoded letters a, from the decoder is dependent
 on the present and past inputs to the encoder and on its past outputs,
 but it is also dependent on the history of the keystate, since the code
 symbol being keyed cannot be changed until the current symbol has com-
 pleted keying.
 The transition probabilities describing the encoder function then are given
 by: 
\end_layout

\begin_layout Standard
(5) P(a k |u k £ k X k-1 a k _ x ^_ ± 
\end_layout

\begin_layout Standard
The evolution of letters from the source is dependent on the history of
 the message text, but it is also dependent on the history of the encoding
 process, since the letter being encoded cannot be changed until the current
 letter has completed the encoding procedure.
 The transition probabilities for the source then are: 
\end_layout

\begin_layout Standard
(6) pU k |X k _l a k-l> - PrU k = L il A k-l " V a k-l = A m ] - 
\end_layout

\begin_layout Standard
The control vector u.
 is modeled as a conditional Markov 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
chain, conditioned on a, , / $, -i / ^v i / accounting for the dependence
 of operator sending peculiarities and data rate on message context, message
 duration, traffic type, etc.
 The transition probabilities for this model are: 
\end_layout

\begin_layout Standard
(7) P(« k lH k -i « k -i e k -i 
\backslash
 k _ x ) = Pr[u k = hJu^ = u j( 
\end_layout

\begin_layout Standard
a k-l " A m' 6 k-l = B n' "Vl = M p ! 
\end_layout

\begin_layout Standard
In terms of the abbreviated notation defined by expressions (4) through
 (7) above, the state transition matrix is given in terms of the components
 of the state vector s, by: 
\end_layout

\begin_layout Standard
P(£ k H k £ k l£ k _i Hfr-i 2k-!) E P<* k ^ a k a k * k 
\backslash
 Hfcl 
\end_layout

\begin_layout Standard
x k-l S k-1 a k-l *k-l X k-1 -k-1 5 
\end_layout

\begin_layout Standard
51 
\end_layout

\begin_layout Standard
Invoking the independence of appropriate variables argued in writing expressions
 (4) - (7), this expression reduces by the chain rule to: 
\end_layout

\begin_layout Standard
(8) p(s.
 u.
 o, K , u.
 ,) = p(xja.
 B., a.
 - ) p(3 v |x, 0, , ) 
\end_layout

\begin_layout Standard
k'"k "k-1 k-1 
\end_layout

\begin_layout Standard
k 1 k ^k-1 
\end_layout

\begin_layout Standard
p(a J £ k *k a k-l X k-1 3 k-l } * P (a kl a k a k-l } 
\end_layout

\begin_layout Standard
P (£ kl X k-l a k-l> ' P (X kl £ k X k-1 } 
\end_layout

\begin_layout Standard
P(H k lu k _! Vl 3 k-l W- 
\end_layout

\begin_layout Standard
Now the expressions for the transition probabilities of 3, , a,, 
\backslash
, are given by the following due to definitions (1) - (3) : 
\end_layout

\begin_layout Standard
P (S kl X k 6 k-l> = 
\end_layout

\begin_layout Standard
1, if B.
 = f Q (K.
 ,B ! i 3 j n 
\end_layout

\begin_layout Standard
0, otherwise 
\end_layout

\begin_layout Standard
p(a kl a k "k-l* = 
\end_layout

\begin_layout Standard
p(A k U k A k _ 1 ) = 
\end_layout

\begin_layout Standard
1, if A- = f (A.
 ,A ) 
\end_layout

\begin_layout Standard
l a j n 
\end_layout

\begin_layout Standard
0, otherwise 
\end_layout

\begin_layout Standard
1, if M.
 = f , (L.
 ,M ) 
\end_layout

\begin_layout Standard
' l A j n 
\end_layout

\begin_layout Standard
0, otherwise 
\end_layout

\begin_layout Standard
52 
\end_layout

\begin_layout Standard
Thus the transition probability (8) is zero for unallowable transitions,
 where the set of allowable transitions is given by (1) - (3) .
 The expressions for the state transi- tion probabilities (8) , then, may
 be written as 
\end_layout

\begin_layout Standard
CSTa) P (s k u k |u k _ x a k _ x ) = 
\end_layout

\begin_layout Standard
p(x kl a k B k-1 a k-l> * P (a k' £ k H-k a k-l X k-1 3 k-l } 
\end_layout

\begin_layout Standard
* pU kl A k-l e k-l } * P ( ^k-1 a k-l S k-1 A k-1 } where the set of allowable
 transitions is given by 
\end_layout

\begin_layout Standard
Ob) f _<*,£*_!> = IVVW t^.^i) f X ( V X k-l» T - 
\end_layout

\begin_layout Standard
Expression (9), then is the desired description of the probabilistic evolution
 of the state of the HKM process, given in terms of the source (message)
 statistics, Morse encoding procedure, keying characteristics and data rate
 statistics.
 
\end_layout

\begin_layout Standard
This model for the HKM process accounts for many effects which go into the
 generation of the key output logic levels.
 The extent to which the model accurately represents a Morse code stream
 is determined by the complexity of the memory functions f, , f , f R and
 by the proper assignment of the conditional transition probabilities.
\end_layout

\begin_layout Standard
For example, if the f, function is sufficiently complex and clever, the
 entire past context of a message may be accounted for in assignment of
 the letter transition probabilities.
 In the simplest case, the assumption is made that f, =0, and uniform probabilit
ies are assigned to the letter transitions.
 The next level of complexity is to assume that f , = ^-i ' a ^ ow i n 9
 a Markov model for the letter transition probabilities.
 Considerably more complex is a model which recognizes that certain sequences
 of letters are always followed by a known sequence in certain formatted
 messages.
 The most sophisticated model for this function is one which models the
 structure of the Morse code message as a natural language, requiring constructi
on of syntatic and grammar-like rules which are used to parse the message
 into meaningful sequences of letters and words.
 Such a model would obviously require a highly complex f,.
 
\end_layout

\begin_layout Standard
At the next level, that of encoding the letters into the mark/space durations
 consistent with the dot/dash/space Morse sequence for the letter, any level
 of sophistication and cleverness for the f function may be used, together
 with the model for the vector control variable u.
 It is at this point that operator inconsistencies such as deletion, substitutio
n and insertion of Morse elements can be accounted for.
 Additionally, by proper construction of the f function, one may also account
 for variations in weight (average dot/elem-space ratio), sending speed,
 and known conditional relationships between the ratios of current to predecesso
r element durations.
 In the simplest case, the assumption is made that the operator always encodes
 perfectly and that his element durations are consistent.
 This simple case would apply to machine-sent Morse code and corresponds
 to the situation where u = constant, and f = a, , .
 
\end_layout

\begin_layout Standard
— a k-1 
\end_layout

\begin_layout Standard
At the key, the durations a, are converted into the 0,1 logic levels of
 duration roughly equal to that produced by the encoder.
 The human, however, cannot always produce these durations consistently;
 thus, the time duration in a particular state will be random, with mean
 value roughly equal to the durations produced by the encoding process,
 and with a variance inversely proportional to his proficiency and concentration.
 There are, for example, certain con- ditional relationships which have
 been found to be true for almost every operator; in particular, inter-element
 dots are more consistently produced than beginning or ending dots.
 
\end_layout

\begin_layout Standard
At this point, also, the effect of the type of key used by the operator
 may be accounted for.
 Hand-keys, mechanical bugs, and electronic bugs all produce different duration
 statistics for the same operator with the same message.
 
\end_layout

\begin_layout Standard
The purpose of this research is not to derive sophis- ticated models for
 the f-f unctions, but to derive a result which shows in general, whatever
 model is used, how the concepts of context, message formatting, operator
 encoding anomalies, and operator "fist" modeling may be included in a unified
 framework to produce at the receiver an optimal 
\end_layout

\begin_layout Standard
55 
\end_layout

\begin_layout Standard
estimate of the transmitted text.
 The extent to which the output translated text is an accurate reproduction
 of the transmitted message is clearly a function of the sophis- tication
 and accuracy of the model used.
 
\end_layout

\begin_layout Standard
The results of this development of the model are summar- ized in the following
 simple theorem.
 
\end_layout

\begin_layout Standard
Theorem 
\end_layout

\begin_layout Standard
Let S, be an n-dimensional discrete-valued random vector 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
with finite state-space: (S.; i = 1,2,...
 N}.
 
\end_layout

\begin_layout Standard
Let (J, be an m-dimensional discrete-valued random vector 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
with finite state-space: {U.; i = 1,2,...M}.
 
\end_layout

\begin_layout Standard
Let Z, be an r-dimensional discrete-valued random vector 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
with finite state-space: {A.; i = 1,2,...R}.
 
\end_layout

\begin_layout Standard
Define the function f : S.
 X I.
 -*• I.
 such that a, = f (s, , a, ,), where s, ,a, are realizations of the random
 processes 5 k /Z k ^ respectively.
 
\end_layout

\begin_layout Standard
Let the probabilistic evolution of the U, process be described by the following
 conditional Markov process: 
\end_layout

\begin_layout Standard
P(u kl u k-1 Q k-1 } " PrCu k = U il U k-l = V a k-l = V 
\end_layout

\begin_layout Standard
all j , m, I .
 
\end_layout

\begin_layout Standard
Let the probabilistic evolution of the S, -process be described by the following
 conditional probabilistic mapping of the U, -Markov process: 
\end_layout

\begin_layout Standard
56 
\end_layout

\begin_layout Standard
p (s kl u k u k-l a k-l } - Pr[s k = s il u k = V u k-l = V 
\end_layout

\begin_layout Standard
a k-l = A n ] ' a11 if i ' l ' n 
\end_layout

\begin_layout Standard
Then, the output state s, of the HKM process described by equation (9) results
 from a probabilistic mapping of the Markov control vector u,, conditioned
 on the entire past history of the output state.
 
\end_layout

\begin_layout Standard
Proof: 
\end_layout

\begin_layout Standard
First, it is clear that the function f records the past history of the output
 state s, , since 
\end_layout

\begin_layout Standard
a k = f a (s k' a k-l } ~ f a (s k' f a (s k-l' a k-2 )} 
\end_layout

\begin_layout Standard
~ f a (s k' f a (s k-l' f a (s k-2' "•• f a (s l' V ) ' * ' } ' 
\end_layout

\begin_layout Standard
Second, expression (9a) reduces by the chain rule to 
\end_layout

\begin_layout Standard
P(s k u kl u k-l a k-l> = ? (s kl u k u k-l 'W ' p(u k' u k-l a k-l } 
\end_layout

\begin_layout Standard
Corresponding the terms on the right-hand side with the S, , U, processes
 described above, and expression (9b) with the f function, the theorem is
 proved.
 
\end_layout

\begin_layout Standard
57 
\end_layout

\begin_layout Standard
Corollary 
\end_layout

\begin_layout Standard
Let the function f be invertible in the sense that 
\end_layout

\begin_layout Standard
a 
\end_layout

\begin_layout Standard
s, = f (o
\backslash
 ,o
\backslash
 ,) is uniquely defined.
 
\end_layout

\begin_layout Standard
K US K K J.
 
\end_layout

\begin_layout Standard
Then the output state o
\backslash
 of the HKM process is a sliding 
\end_layout

\begin_layout Standard
block encoding of the sequence s n ,s,,Sp ...
 s, , where the 
\end_layout

\begin_layout Standard
evolution of the S, process is described by the conditional mapping: 
\end_layout

\begin_layout Standard
p(s lJ u k-l a k-l } " Pr[s k = S il u k-1 = V a k-l = A m ] 
\end_layout

\begin_layout Standard
and the U process is described by: 
\end_layout

\begin_layout Standard
p (u kl u k-i a k-i a k ) = Pr[u k = u il u k=i = u j' a k-i = V 
\end_layout

\begin_layout Standard
a, = A ] .
 k n 
\end_layout

\begin_layout Standard
Proof: From the main theorem, the state a, is describeable 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
as : 
\end_layout

\begin_layout Standard
a k = V s k' f a (s k-l' f a (s k-2' •• f a (s l' )) "- ) ' 
\end_layout

\begin_layout Standard
which can be expressed in terms of a new function f as 
\end_layout

\begin_layout Standard
a k f a (s k' S k-l' s k-2'** - s i' a 
\end_layout

\begin_layout Standard
58 
\end_layout

\begin_layout Standard
Now, defining a Q = s Q , which is consistent with (9b) since o_, is arbitrary,
 then f ' represents a sliding block encoding of the sequence {s.}, i = 0,1,...
 k.
 Now (9a) can be expressed as: 
\end_layout

\begin_layout Standard
p(s k u kl u k-l a k-l> = P (u kl U k-l a k-l s k } ' P (s kl u k-1 a k-l
 } 
\end_layout

\begin_layout Standard
and by the corollary hypothesis on the invertibility of f , 
\end_layout

\begin_layout Standard
= p(u k |u k _ 1 o^ f^io^a^) • P(s k |u k _ 1 a k _ 1 ) But u, is already
 conditioned on a, .
 , so the additional conditioning provided by s, = f (g,,g, ,) is exactly
 
\end_layout

\begin_layout Standard
K.
 OS JC jC^J.
 
\end_layout

\begin_layout Standard
that provided by g, , thus (9a) is reduced to: 
\end_layout

\begin_layout Standard
P (s k u kl u k-i a k-i } E P (u kl u k-i a k-i a k } * ? (s kl u k-i a
 k-i } ' 
\end_layout

\begin_layout Standard
which are the two processes hypothesized, proving the corollary.
 
\end_layout

\begin_layout Standard
Comments : The theorem and corollary are interesting pri- marily from a
 theoretical viewpoint.
 The main theorem actually does no more than place the intuitively developed
 model for the HKM process on a solid probabilistic founda- tion.
 In Section V, where an optimal estimator for the state of the process is
 derived through Bayesian techniques, the form of the model presented in
 the main theorem is that which is used.
 However, after the estimation algorithm has 
\end_layout

\begin_layout Standard
59 
\end_layout

\begin_layout Standard
been derived, it is shown that the optimal estimator has a trellis structure,
 which is not surprising in view of the corollary result showing an SBC
 interpretation.
 The block diagram shown in Figure 9 is useful for visualizing the evolution
 of the output state, s, .
 
\end_layout

\begin_layout Subsection
BASEBAND HKM CHANNEL MODEL 
\end_layout

\begin_layout Standard
Although the channel model for the HKM process described in Section III
 was useful for obtaining lower bounds an error-rate performance, it is
 of little use in actually describing the physical processes which affect
 the reliable transmission of a Morse message.
 Consider the following simplified model of the communication channel for
 Morse transmitted at HF.
 The keyer turns the transmitter on and off according to the HKM source.
 When keyed, the transmitted RF signal has amplitude C(t) at a carrier frequency
 u).
 The HF propagation channel introduces both additive noise (N(t)) in the
 form of atmospherics and interference, and multipli- cative noise (B(t))
 in the form of fading and multipath propagation effects.
 At the receiver, the carrier is removed after being band-pass filtered
 and gain-controlled.
 After low-pass filtering and sampling, the baseband signal is given by
 z, = x, c, b, + n, , where c, is the sampled, gain-controlled received
 signal amplitude; b, is the sampled, gain-controlled, low-pass filtered
 effective multiplicative noise component; and n, is the low-pass filtered
 version of the additive noise.
 
\end_layout

\begin_layout Standard
60 
\end_layout

\begin_layout Standard
of' 
\end_layout

\begin_layout Standard
G 
\end_layout

\begin_layout Standard
61 
\end_layout

\begin_layout Standard
The sampled version of the amplitude of the transmitted carrier c, is a
 constant value while x,= 1 .
 During the period when x, = , the amplitude will remain constant at the
 same value as for x, = 1 for a large percentage of the time.
 However, it is not uncommon for the operator to go into a pause during
 which time he readjusts the transmitter power either up or down.
 These adjustments are usually made between messages, but also can occur
 during a short pause between letters.
 Thus the signal carrier amplitude is a random variable with a transition
 probability density which is conditioned on the memory of the HKM process
 and the current key state.
 In the simplest case, the model may be made conditional only on x, and
 x, _, , having, as a con- sequence, the result that the carrier amplitude
 is allowed to change randomly during every 0-state duration.
 More realistically, one level of complexity greater allows the transition
 probability to be conditioned on 3, .
 such that the amplitude can change only when 3, i indicates a pause.
 
\end_layout

\begin_layout Standard
The effect of transmitter power fluctuations at the output of the receiver
 is dependent on SNR and on the AGC employed for gain-leveling.
 For moderate to high received SNR, the effective c, observed at the receiver
 output stays relatively constant because of AGC action.
 However, when noise power becomes a significant portion of the total power
 controlling the AGC, then c, varies nearly the same as C, .
 Thus an efficient model of transmitter power fluctuations must take 
\end_layout

\begin_layout Standard
62 
\end_layout

\begin_layout Standard
into consideration not only the actual power variations of the transmitter,
 but also the effect of the receiver RF , IF, and AGC sections as well.
 
\end_layout

\begin_layout Standard
Consider now the multiplicative noise term, which has the observable effect
 of varying signal amplitude.
 If it arises because of relatively slow fading, then its effect will be
 cancelled by the combination of AGC and low-pass filtering.
 If, on the other hand, it is caused by fast fading (perhaps due to multipath)
 , then the AGC cannot respond fast enough to keep the output signal-level
 constant On an 00K signal, the effect is the same as if the trans- mitter
 power were changed during the carrier off-time.
 
\end_layout

\begin_layout Standard
The term c,b, , then, represents an effective transmitter power fluctuation,
 dependent on both the HKM process and the HF channel, with the result that
 the marks of the HKM process appear to be transmitted with random amplitude.
 During the period of a MARK, the effective fluctuations are caused by the
 slow fading component with intensity and rate determined by the channel,
 the AGC, and the low-pass filter.
 
\end_layout

\begin_layout Standard
In view of the above consideration, it is appropriate to model the apparent
 transmitted amplitude y, as a condi- tional gauss-Markov process, dependent
 on both the HKM process, and the channel: 
\end_layout

\begin_layout Standard
(10a) y(k) = YF(s k a R _ 1 ) y(k-l) + f(s k a k-1 ) w fc (k) 
\end_layout

\begin_layout Standard
63 
\end_layout

\begin_layout Standard
where w (k) is a zero-mean gaussian random sequence with unit variance;
 
\end_layout

\begin_layout Standard
F(s, cj,,) is a function of the state of the HKM source; 
\end_layout

\begin_layout Standard
T(s, cr,,) is a similar function, 
\end_layout

\begin_layout Standard
Y is a channel-dependent fading parameter.
 
\end_layout

\begin_layout Standard
Now, since the amplitude is observed only during a MARK period, the measurement
 equation is given by: 
\end_layout

\begin_layout Standard
(10b) z k = x k y k + u k , 
\end_layout

\begin_layout Standard
where n, is the low-pass filtered, gain-controlled channel noise.
 
\end_layout

\begin_layout Standard
Equations (10) represent the described HKM Baseband channel model, which
 accounts for the effects of fading on an OOK signal and the effect of actual
 transmitter power fluctuations caused by the sending operator.
 
\end_layout

\begin_layout Standard
Generalizing these intuitive concepts to a vector channel results in the
 following channel-measurement model.
 Consider that the output sequence s, of the HKM is observed through the
 following channel and measurement processes : 
\end_layout

\begin_layout Standard
y k = * (s k ff k-l ) y k-l + r(s k a k-l } w ) 
\end_layout

\begin_layout Standard
z k = H(s k } y k + n k 
\end_layout

\begin_layout Standard
64 
\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
y, is a p-dimensional state vector; 
\end_layout

\begin_layout Standard
z, is a q-dimensional measurement vector; 
\end_layout

\begin_layout Standard
J(s, a ] c _i) is a p x p state transition matrix; 
\end_layout

\begin_layout Standard
H(s, ) is a q x p measurement matrix; 
\end_layout

\begin_layout Standard
T(s, a, _, ) is a p x p matrix; 
\end_layout

\begin_layout Standard
w, is a p-dimensional plant noise vector; 
\end_layout

\begin_layout Standard
n, is a q-dimensional measurement noise 
\end_layout

\begin_layout Standard
vector ; 
\end_layout

\begin_layout Standard
w, is statistically independent of w ? for I ^ k; n, is statistically independen
t of n ? for t ^ k; w, is statistically independent of n, ; p (y ) /P (w,
 ) ,p (n, ) are given probability densities.
 
\end_layout

\begin_layout Standard
It is to be noted that this observation model, when con- ditioned on s,
 ,a, ,, is linear.
 Further if the probability densities are gaussian, then the s, o,_-, -
 conditional estimate of y, , given the sequence z, , k = 1,2,..., is given
 by the well-known Kalman filter recursions.
 
\end_layout

\begin_layout Standard
65 
\end_layout

\begin_layout Section
THE ESTIMATION PROBLEM 
\end_layout

\begin_layout Standard
The estimation problems of interest, based on the HKM source, channel, and
 measurement models, can be divided into two broad classes.
 The first results when the HKM transition and mapping probabilities are
 known a-priori for all k; the problem then is to find an optimal (in some
 sense) estimator for s, and/or u, given noisy observations.
 It will be shown that the desired estimator is not physically realizable
 in general because it requires an exponentially expanding memory.
 In Section VIII, however, practical realizations of a suboptimal estimator
 are discussed, and it is shown that one can systematically come as close
 to optimal estimation as desired.
 The second class of estima- tion problems results when the HKM model probabilit
ies are known only to the level of an initial probability distribution.
 The problem here is to estimate s, and/or u, and the transition and mapping
 probabilities themselves.
 Only the first class will be treated here.
 
\end_layout

\begin_layout Standard
In this class of estimation problems, the transition and mapping probabilities
 are specified, and the problem is to estimate the state of the system at
 time k, given the sequence of all past measurements z = {z, , z 2 , .
 .
 .
 , z, } .
 The state estimate of the system is given by the joint estimate of the
 output, control, and memory states s, u, a, .
 The problem of obtaining an optimal estimate of the state is approached
 in the traditional manner; that is, the (posterior) conditional probability
 distribution p(s, u, a, | z ) is determined for all k, and a suitable optimalit
y criterion is applied to this distribution to arrive at an optimal estimator.
 
\end_layout

\begin_layout Standard
Using the Bayesian approach to the problem of obtaining the posterior distributi
on, a recursive form for the estimator is obtained.
 It will be shown that the resulting structure can be realized by a set
 of simpler, identical filters, operating on a tree or trellis.
 In the case of parameter-conditional linear-gaussian observation and measuremen
t models, these "elemental" filters are Kalman filters.
 In case the observation and/or measurement models are not linear-gaussian,
 then the body of knowledge on non-linear filtering can be brought to bear
 on the design of these elemental filters.
 
\end_layout

\begin_layout Subsection
ESTIMATOR DERIVATION 
\end_layout

\begin_layout Standard
In the following it will be necessary to keep track of both the time index,
 k, and the state value indices for the states s, e {S.}, u, e {U.}, cr, e
 ^o^' To reduce the notational burden which would result from the explicit
 notation of probability statements such as Pr[s, = S.lu, = U.,u.
 , = U ,a, , = A ] .
 the following k 1 ' k j k-1 m k-1 n abbreviated notation will be used.
 The subscript k is the time index, and the superscript is the index of
 the set of state values.
 When k is used as a superscript, it refers to the time sequence of values,
 0,1,2,.
 ..,k; e.g., 
\end_layout

\begin_layout Standard
67 
\end_layout

\begin_layout Standard
k A z = z n z 
\end_layout

\begin_layout Standard
1*2 * * ' 
\end_layout

\begin_layout Standard
Zj.
 Additionally the vector notation using an underbar will be dropped, with
 the understanding that all variables are implicitly vector-valued.
 In terms of this notation, the HKM signal and observation models are: 
\end_layout

\begin_layout Standard
(11) Output State Mapping probabilities: 
\end_layout

\begin_layout Standard
p(S kl U k U k-1 a k-l } = Pr[s k " S il U k = U j' U k-l = U m' a k-1 =
 A q ] 
\end_layout

\begin_layout Standard
(12) Control State Transition probabilities: 
\end_layout

\begin_layout Standard
p(u kl U k-l a k-l } " PrCu k = U jl U k-l = V a k-1 = A q ] 
\end_layout

\begin_layout Standard
(13) Memory: 
\end_layout

\begin_layout Standard
at = f (slrO? , ) = f (S.
 ,A ) k a k' k-1 a l q 
\end_layout

\begin_layout Standard
(14) Channel: 
\end_layout

\begin_layout Standard
^k = ^ (s k 4-l ] y k-l + r(s k ff k-l J W 
\end_layout

\begin_layout Standard
(15) Measurement: 
\end_layout

\begin_layout Standard
Z k = H(s k } y k + V 
\end_layout

\begin_layout Standard
The well-known Bayesian procedure (see, for example, Lee [8] ) for recursively
 determining the posterior density (distribution) is given as follows.
 At time k-1 , the mixture density: 
\end_layout

\begin_layout Standard
.
 n m q | k-1.
 , i n m q k-1 
\end_layout

\begin_layout Standard
p(y k-l s k-l u k-l G k-l' z > = p(y k-ll S k-l U k-1 a k-l ;z 
\end_layout

\begin_layout Standard
, n m q I k-1.
 p(s k-l U k-1 a k-l' z } 
\end_layout

\begin_layout Standard
has been obtained.
 The density at time k, after receipt of a new measurement z, , is given
 by Bayes ' rule: 
\end_layout

\begin_layout Standard
, i i j I k-1.
 .
 i j 1 1 k-1.
 
\end_layout

\begin_layout Standard
,.-, , i j a, k, p (z k' y k s k u X 2 )p(y k s k u k Q k' 2 } 
\end_layout

\begin_layout Standard
(16) P(y k s k u k a k |z ) = k _ 
\end_layout

\begin_layout Standard
P(zJ z ) 
\end_layout

\begin_layout Standard
where: 
\end_layout

\begin_layout Standard
(17) p(y k s k UjJ.
 a k |z " ) = 
\end_layout

\begin_layout Standard
.
 i j I i n m q k-1.
 
\end_layout

\begin_layout Standard
y/ p(y k S k u k a kl y k-l s k-l U k-1 a k-l ;Z 
\end_layout

\begin_layout Standard
yjc-i 
\end_layout

\begin_layout Standard
/ n m q i k-1.
 , 
\end_layout

\begin_layout Standard
* p(y k-l S k-1 U k-1 a k-ll Z } dy k-l 
\end_layout

\begin_layout Standard
(18) p(z k |z k 1 ) = 
\end_layout

\begin_layout Standard
v , i j £,.
 k-1, , | i j I k-1, , 
\end_layout

\begin_layout Standard
ii j p(y k s k u ic a kl z )p(z kl y k s k u k V z } dy k 
\end_layout

\begin_layout Standard
69 
\end_layout

\begin_layout Standard
The desired state posterior probability distribution then is obtained from
 (16) by integrating over y.
 : 
\end_layout

\begin_layout Standard
(19) p(s£ uj o k |z ) = J p(y k s^ u^ a k |z ) dy k .
 
\end_layout

\begin_layout Standard
^k 
\end_layout

\begin_layout Standard
k-1 Substituting expression (18) for p(z, z ) into (16), 
\end_layout

\begin_layout Standard
expression (19) becomes: 
\end_layout

\begin_layout Standard
i j l
\backslash
 K _ 
\end_layout

\begin_layout Standard

\backslash
 
\end_layout

\begin_layout Standard
/i i J J< k-l x , i j %
\backslash
 k-1, - 
\end_layout

\begin_layout Standard
p ( 
\backslash
K s k K °k z ' p(y k s k "k 
\backslash
i z ' dy k 
\end_layout

\begin_layout Standard
(20) p(s k u^o k |z)= f i j £.
 k-i , .
 i j l k-i 
\end_layout

\begin_layout Standard
& J p(y k s k "k 
\backslash
i z » p(z ki y k 
\backslash
 °k a k z ' dy k 
\end_layout

\begin_layout Standard
y k 
\end_layout

\begin_layout Standard
and the problem is to obtain a result for the integral over y, in terms
 of the prior density at time k-1, and the model 
\end_layout

\begin_layout Standard
transition probabilities.
 
\end_layout

\begin_layout Standard
-I V If — I 
\end_layout

\begin_layout Standard
The first term in the integrand, p (z, |y, s, ur_ a, z ), is readily determined
 from the measurement equation (15) and the density of the noise, p (n,
 ) .
 In the case of n k a white sequence, the density is given simply by: 
\end_layout

\begin_layout Standard
(21) P(z k |y k Sk " u£ a£ z*" 1 ) e P (z k |y k s£) - P R (z k - H(s£)y
 k ) 
\end_layout

\begin_layout Standard
The second term in the integrand is given by (17) in terms of the prior
 density and the transition probabilities.
 Rewriting the mixture densities in (17) in terms of the component conditional
 density for y k and the discrete distributions for s, u, a, , expression
 (17) becomes: 
\end_layout

\begin_layout Standard
70 
\end_layout

\begin_layout Standard
Now since s, u, a, are independent of y, , , the density on line (c) above
 is not changed by writing: 
\end_layout

\begin_layout Standard
, , , i n m a k-1.
 .
 i i j % n m q k-1.
 (e) P(y k .
 1 ls k .l Vl a k-l ;z } = p(y k-l |s k "k a k 
\backslash
-l Vl a k-l ;z } 
\end_layout

\begin_layout Standard
Also, by virtue of this independence, the expression on line (b) becomes:
 
\end_layout

\begin_layout Standard
, ,-, .
 i j £, n m q k-1, _ , i j I, n m q .
 (f ) p(s k u£ a k |y s k _ 1 Vl a^jz ) = P^ ^ c^s^ u^ a^) 
\end_layout

\begin_layout Standard
Combining (a) & (e) , substituting (f ) for (b) , and rearranging the terms
 of (22), the expression becomes: 
\end_layout

\begin_layout Standard
71 
\end_layout

\begin_layout Standard
i j % | k-1, P (y k S k u k a kl Z ) = 
\end_layout

\begin_layout Standard
~ , i j £.
 n m q , .
 n m q i k-1.
 S p(s k u R ajs^ u k _ x a^) p(s k _ 1 u k _ 1 a* |z ) 
\end_layout

\begin_layout Standard
nmq 
\end_layout

\begin_layout Standard
• I , | i j £ n m q k-1, 
\end_layout

\begin_layout Standard
J P(y k y k-ll S k U k a k S k-1 U k-1 a k-l ;Z > dy k-l 
\end_layout

\begin_layout Standard
y k-l 
\end_layout

\begin_layout Standard
Carrying out the integration over y^._-i / and noting that y, is not dependent
 on u, a, s, , u v_i ' ^ e desired result for expression (17) , in terms
 of the prior and transition probabilities, is given by: 
\end_layout

\begin_layout Standard
(23) p(y k s k u^ a k |z ' ) = 
\end_layout

\begin_layout Standard
„ .
 i j £.
 n m q , , n m q .k-1.
 E P (s k uJ a k |s k-1 u k _ x ag_ 1 ) p(s k _ x u^ a^ | z ) 
\end_layout

\begin_layout Standard
n,m,q 
\end_layout

\begin_layout Standard
/ i ± <g k-1, * p(y kl S k a k-l ;Z } * 
\end_layout

\begin_layout Standard
The integral in (20) is then given in terms of (23) and (21) as: 
\end_layout

\begin_layout Standard
, i i j I k-1, , i j £.
 k-1, , (24) / P(z k |y k s k u£ a k z ) p (y k s R u^ a k | z ) dy R =
 
\end_layout

\begin_layout Standard
y 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
„ , i j I i n m q n , n m a .k-1.
 Z P (s k u^ a k |s k _ 1 u k _ x ag_ 1 ) P (s k _ 1 u^ a^ | z ) 
\end_layout

\begin_layout Standard
nmq 
\end_layout

\begin_layout Standard
f 
\end_layout

\begin_layout Standard
} i i i
\backslash
 / i ! ^ k-1 
\end_layout

\begin_layout Standard
^k 
\end_layout

\begin_layout Standard
/i ix / i i q k-1, - P(z k |y k s k ) P(y k |s k crg„ i; z ) dy k 
\end_layout

\begin_layout Standard
72 
\end_layout

\begin_layout Standard
The resulting integral over y, in the above expression is seen to be a likelihoo
d function since 
\end_layout

\begin_layout Standard
// i i
\backslash
 / i i q k-1, .
 | i q k-1, P(z k ly k s k ) P(y k |s k ag.^z ) = P(z k |s k a£_ 1? z ) 
\end_layout

\begin_layout Standard
Denoting this integral, then, as the likelihood, 
\end_layout

\begin_layout Standard
, n _, , iq A / , I i.
 , I i q k-1, , 
\end_layout

\begin_layout Standard
(25) L^ = ;p(^ k ly k s k ) P(y k |s k erg ,z ) dy k , 
\end_layout

\begin_layout Standard
y 
\end_layout

\begin_layout Standard
k the posterior conditional density (20) is given by (24) & (25) as 
\end_layout

\begin_layout Standard
„ , i .i i | n m q , , n m q i k-1.
 , iq I P(s k u£ o k I s k _ 1 V] _ a^_ 1 ) P (s k _ 1 Vl a^lz )L^ 
\end_layout

\begin_layout Standard
- .
 nmq 
\end_layout

\begin_layout Standard
26) p (Sk ^ ak ' z } 7.
 z p (s k "k a k I s k-i Vi a k-i } p (s k-i Vi a k-i I zk ~ 1} L k q 
\end_layout

\begin_layout Standard
13 nnq 
\end_layout

\begin_layout Standard
This is the desired result for the recursive calculation of the probabilities
 of the states s, u k a, given the measurement k sequence z .
 In terms of the model transition probabilities (11) and (12) and the memory
 function (13), the transition probabilities are computed as: 
\end_layout

\begin_layout Standard
, i j 1 1 n m q , _ p(s k u k a kl s k-l u k-l G k-1 } = 
\end_layout

\begin_layout Standard
, ii j m q 
\backslash
 , j 1 m q 
\end_layout

\begin_layout Standard
p (s kl u k u k-l a k-l } p (u i>k-l a k-l 
\end_layout

\begin_layout Standard
73 
\end_layout

\begin_layout Standard
along the allowable transition paths specified by 
\end_layout

\begin_layout Standard
a k = V s k a k-i } - 
\end_layout

\begin_layout Standard
For each memory state and control state value at time k-1, the transition
 probability p(u?|u, , cj^_, ) is specified by (12) for all j,m,q.
 Then for each j,m,q, the mapping probability p(s, |u? u, _ , cr?-,) is
 given for all i by (11); the value for a, is found for each i,q pair by
 (13) , and L, q is computed by (25) .
 The posterior probabilities are then computed by (26) and the state values
 and their probabilities are in place for the next recursion.
 
\end_layout

\begin_layout Standard
Clearly the ability to carry out the recursion (26) exactly depends on whether
 or not the likelihood (25) can be found in closed form.
 Such a form can indeed be found for the linear channel and measurement
 models (14) and (15) in case the noise n, is white and gaussian, as will
 now be shown.
 
\end_layout

\begin_layout Standard
First note that the densities involved in the expression for the likelihood
 (25) are both conditioned on specific realizations of s, and a _.
 , namely s, = S .
 and cr, , = A .
 The first density p(z, |y, s, ) is given by (21) for the white noise sequence;
 for the white gaussian sequence, (21) becomes 
\end_layout

\begin_layout Standard
(27) PU k ly k s£) = p n (z k - H(s£)y(k)) = N z (H(s^)y(k) ,R) , 
\end_layout

\begin_layout Standard
where N (m,V) is the gaussian density with mean x = m, 
\end_layout

\begin_layout Standard
variance V and p (n, ) = N (0,R) .
 
\end_layout

\begin_layout Standard
^n k n.
 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
74 
\end_layout

\begin_layout Standard
Consider now the second density in the integrand (25) , p (y, | s, a?_,;z
 ), the s, a T,_-i ~ conditional one-step prediction density for y, , along
 the path specified by the S.
 transition at time k from the memory state A at time i *- q k-1.
 The path label, then, at time k, resulting from the extension of the path
 labeled A at time k-1, is A„ = f (S.
 ,A ) .
 Now I o l' q 
\end_layout

\begin_layout Standard
.
 | i q k-1.
 / , i i q k-1, P ( *kl S k Q k-l ?Z > = J P(y kl y k-1 S k a k-l ;Z > 
\end_layout

\begin_layout Standard
y k-l 
\end_layout

\begin_layout Standard
/ i i q k-1.
 ' P (y k-ll S k a k-l ;Z > dy k-l' 
\end_layout

\begin_layout Standard
and since the s, a?_, pair is uniquely embodied in a, = f (s, a^ n ) , and
 y, .
 given z is independent of s, , the above expression becomes 
\end_layout

\begin_layout Standard
/~,r,x i i & k-1, / , i i XT k-1, 
\end_layout

\begin_layout Standard
(28) P(y k |a k ;z )= J p(y k |y k-1 s k ag_ 1?z ) 
\end_layout

\begin_layout Standard
y k-l 
\end_layout

\begin_layout Standard
, i q k-1, , 
\end_layout

\begin_layout Standard
for each a, along a path given by I = , i q N a k = f a (s k' a k-l } -
 
\end_layout

\begin_layout Standard
Now when the a-conditional density for the initial value of y, is gaussian
 and the s, a, _-, - conditional channel model is linear gaussian , the
 above density (2 8).
 is gaussian for all k, and the mean and variance of the density is given
 by the Kalman filter recursions.
 Specifically, this density is given by 
\end_layout

\begin_layout Standard
(29) p( Yi X ^ _1 ) = N y <y k |k-l ( V' V k|*-l<V> 
\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
^k|k-l ( V = * (S i V ^k-l|k-l (A q ) 
\end_layout

\begin_layout Standard
V.
 I, ,(A.) = $(S.
 A ) V, - i.
 .
 (A ) <£ T (S.
 A ) + Q, (S.
 A ) k 'k-1 V - 1 q k-l|k-l q - l q ^k i q 
\end_layout

\begin_layout Standard
A £ = f a (S i y 
\end_layout

\begin_layout Standard
and the recursions for y, i , ( • ) and V, i, (•) are given by the remaining
 Kalman filter equations: 
\end_layout

\begin_layout Standard
^ik ( v =^ik-i (A £) + G k<v [z k- H(s i»ykik-i ( V ] 
\end_layout

\begin_layout Standard
V klk ( V " (I-S (A £» H(S i )) V klk-l ( V 
\end_layout

\begin_layout Standard
w = v kik-i (A X< s i )[H(s i ,v kik-i ( v HT(s i> + R k rl 
\end_layout

\begin_layout Standard
Substituting these expressions (27) and (29) back into (25), the integral
 to evaluate becomes: 
\end_layout

\begin_layout Standard
76 
\end_layout

\begin_layout Standard
L i q = J N 2 « H < s i»y k 'V • N y v ( ^ik-i ( V' v k|k-i ( V' a y k '
 
\end_layout

\begin_layout Standard
The evaluation of this integral is a basic exercise in integration of gaussian
 densities and is given by [8] : 
\end_layout

\begin_layout Standard
v 1/2 
\end_layout

\begin_layout Standard
(29) L* = c " " ' lr - " - T -" 
\end_layout

\begin_layout Standard
iq 
\end_layout

\begin_layout Standard
V (A 
\end_layout

\begin_layout Standard
z k|k-l * 
\end_layout

\begin_layout Standard
E *P { -2 [z k|k-l ( V ] [V z v|v <V ] 1 k k-1 
\end_layout

\begin_layout Standard
[z kik-i<V> 
\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
z k | k -i ( V z k - H(s i> y k ik-i< A i» 
\end_layout

\begin_layout Standard
V z v|v «V  H(S i» V k|k-l (A ^ HT(S i» + 
\backslash
 
\end_layout

\begin_layout Standard
k k-1 ' 
\end_layout

\begin_layout Subsection
B.
 IMPLEMENTATION STRUCTURE OF ESTIMATOR 
\end_layout

\begin_layout Standard
The structure of the filter realization density (26) , together with the
 likelihood calculation (29), is that of a tree with nodes given by the
 past state trajectories and with branches labeled by the states of process.
 For each transition, i.e., each path extension to a new node, the likelihood
 of the transition is computed from the Kalman filter recursions along that
 particular path.
 The likeli- hoods are multiplied by the transition probability for that
 path extension, and by the previous path probability.
 The updated path probabilities are then obtained by normalizing these products.
 The tree structure showing the evolution of the path labels according to
 a particular function is illustrated in Figure 10.
 The next stage of this structure would obviously contain N x I, nodes where
 N is the number of possible states S.
 and I, is the number of nodes at stage k.
 Thus the number of nodes expands exponentially.
 However, in case the function f depends only on a finite portion of the
 past trajectory, then the tree structure eventually becomes a finite trellis
 at the stage which accounts for the definition of f , resulting in a trellis
 appropriate for Viterbi decoding.
 If the function f has infinite memory, then obviously some approximation
 technique must be used to keep the number of nodes finite.
 One such possible approximation is to save only a given number of nodes
 at each stage, most likely those with the highest posterior probability.
 Another scheme which is possible is to save only enough nodes at each stage,
 the sum of whose posterior probabilities is less than or equal to some
 specified number, P ^.
 This latter method is attractive from the standpoint that for high signal-to-no
ise ratios the number of nodes saved would be small, while for low SNR,
 the number saved would be larger.
 This scheme therefore would have the attractive feature that the processing
 load would automatically adapt to the SNR.
 
\end_layout

\begin_layout Standard
k+l 
\end_layout

\begin_layout Standard
FIGURE 10.
 Estimator Structure 
\end_layout

\begin_layout Standard
79 
\end_layout

\begin_layout Subsection
ESTIMATOR ALGORITHM 
\end_layout

\begin_layout Standard
The following algorithm implements the estimator given by equations (26)
 and (29).
 For a practically realizable estimator, some rule which saves only a finite
 number of paths as discussed above must be used at step 8.
 
\end_layout

\begin_layout Standard
Step Initialization: k = 
\end_layout

\begin_layout Standard
I = MN (number of joint S, ,u, states) 
\end_layout

\begin_layout Standard
A (i) , i = 1,2,..., I , arbitrarily specified 
\end_layout

\begin_layout Standard
P°(i) = 1/MN, i = 1,2,.
 ..
 ,1° 
\end_layout

\begin_layout Standard
Step 1 Obtain indices for new nodes a) k = k + 1 
\end_layout

\begin_layout Standard
b) For q = 1,2,...
 1^ ' 
\end_layout

\begin_layout Standard
m = 1,2,...
 M n = 1,2, ..
 .
 N 
\end_layout

\begin_layout Standard
j = (q-1) I (k X) + (m-l)M + n 
\end_layout

\begin_layout Standard
Step 2 Label each new node: For each n, m, q, obtain 
\end_layout

\begin_layout Standard
A k (j) = f a (S m ,A k l (q)) 
\end_layout

\begin_layout Standard
80 
\end_layout

\begin_layout Standard
Step 3 Obtain transition probabilities: For each n, m, q, obtain 
\end_layout

\begin_layout Standard
PTR(m, n, q) = PS (S lu ,U , A k 1 )-PR(U 1 |U / A k ~ 1 ) ^ m 1 n' q' q
 k'q'q 
\end_layout

\begin_layout Standard
Step 4 Calculate L for each hypothesized transition 
\end_layout

\begin_layout Standard
(some obvious indices are omitted! 
\end_layout

\begin_layout Standard
For each n, m, q, compute: a) Kalman step: 
\end_layout

\begin_layout Standard
W-i^> = *(s ra A k_1 (q)) ? k -i| k -ite> 
\end_layout

\begin_layout Standard
v kik-i ( 3) *< s m ^W'Viik-i^ + V 8 ..
 Ak " 1(q)) 
\end_layout

\begin_layout Standard
G k ( 3> = v kik-i ( 3 )HT(s m» [HV kik-i HT + V' 1 
\end_layout

\begin_layout Standard
z k|k-l ( 3»  z k " H(S m» ^Ik-l^' 
\end_layout

\begin_layout Standard
yklk (j) = y klk-l (j) + G k ( 3» Z K |k-l (j) 
\end_layout

\begin_layout Standard
v kik ( 3> = (I - G k ( 3) H ' s m ) >v klk-i ( 3' 
\end_layout

\begin_layout Standard
V z v ,v , (j> = H(S m )V k|k-l^ )HT + R k 
\end_layout

\begin_layout Standard
k k-1 ' 
\end_layout

\begin_layout Standard
81 
\end_layout

\begin_layout Standard
Step 8 Update number of paths 
\end_layout

\begin_layout Standard
i< k > = mi*- 11 
\end_layout

\begin_layout Standard
go to step 1.
 
\end_layout

\begin_layout Standard
It is to be noted that the computations cannot be 
\end_layout

\begin_layout Standard
carried out "in place"; that is, A (j) cannot be stored in 
\end_layout

\begin_layout Standard
k-1 k 
\end_layout

\begin_layout Standard
the same locations as A (j) until all the A (j) have been 
\end_layout

\begin_layout Standard
computed.
 Similarly, the Kalman filter means and variances 
\end_layout

\begin_layout Standard
must be stored in separate temporary locations until step 5 
\end_layout

\begin_layout Standard
is completed.
 
\end_layout

\begin_layout Subsection
DISCUSSION AND RELATION TO PREVIOUS RESULTS 
\end_layout

\begin_layout Standard
In the language of the literature on non-linear filtering, the present result
 represents an extension of previous results in system identification problems
 to the case where the unknown discrete system parameter s, is the result
 of a probabilistic mapping of an underlying memory-conditional Markov process.
 Previous investigations have treated both the case where s, is a Markov
 process [10], [11], and the case for s, an unknown time-invariant parameter
 [9].
 The present result reduces to these results for the appropriate modeling
 of s, .
 
\end_layout

\begin_layout Standard
Case I: Markovian Parameters [10] [11] 
\end_layout

\begin_layout Standard
In this case, S, is a finite-state discrete- 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
time Markov chain with transition matrix 
\end_layout

\begin_layout Standard
A (P..(k)} = {Pr[s, = S.|s , = S.]}.
 The n-dimensional , 
\end_layout

\begin_layout Standard
1 j K 1 }C — _L 
\end_layout

\begin_layout Standard
S-conditional system dynamics are given by: 
\end_layout

\begin_layout Standard
82 
\end_layout

\begin_layout Standard
*k - *<V* k -i + r(S k )w k-i 
\end_layout

\begin_layout Standard
and the m-dimensional measurements are 
\end_layout

\begin_layout Standard
z k  H(S k>yk + n k 
\end_layout

\begin_layout Standard
The random variables w, , n, are zero-mean independent gaussian, and independent
 of the Markov chain S, .
 
\end_layout

\begin_layout Standard
In terms of the generalized model developed above, the 
\end_layout

\begin_layout Standard
memory function f (13) is specified, for this case, by 
\end_layout

\begin_layout Standard
T a, = [s, s t,_-| • • • s ] an d tne output state mapping 
\end_layout

\begin_layout Standard
probabilities (11) are independent of the U, - process and given by {p.
 .
 (k)}.
 The system dynamics and measure- ment equations, in terms of the realization
 of the S, - process are then given by 
\end_layout

\begin_layout Standard
^k = $ (s k a k-l )y k-l + r(s k a k-l )w k 
\end_layout

\begin_layout Standard
z k = H(s k a k-l )y k + n k 
\end_layout

\begin_layout Standard
The posterior measurement-conditional path probabilities 
\end_layout

\begin_layout Standard
are given exactly by equation (26).
 The likelihood equations 
\end_layout

\begin_layout Standard
(29) for L.
 are obtained in the same manner by replacing 
\end_layout

\begin_layout Standard
H(S.) with H(S.
 A ) where A is a path specification obtained l i q q ^ r 
\end_layout

\begin_layout Standard
through the memory function: A = [S.
 (k ~ 1) S : k ~ ) ...
 S^ 0) ] .
 The posterior probability for the parameter s, , then is given by summing
 over the paths : 
\end_layout

\begin_layout Standard
83 
\end_layout

\begin_layout Standard
k A M k 
\end_layout

\begin_layout Standard
P (S.) = Pr[s.
 = S.
 ] = S P i k l , lq 
\end_layout

\begin_layout Standard
q=l ^ 
\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
P k = Pr[s.
 = S.
 ;a.
 = A |z k ] .
 iq k 1 k q 
\end_layout

\begin_layout Standard
The CME or MAP estimate may then be obtained: 
\end_layout

\begin_layout Standard
N .
 
\end_layout

\begin_layout Standard
CME: s, = I s.
 P (S.) 
\end_layout

\begin_layout Standard
k .
 , l l 1=1 
\end_layout

\begin_layout Standard
k k 
\end_layout

\begin_layout Standard
MAP: s.
 = S.: P (S .
 ) = max P (S.) k 3 3 L i 
\end_layout

\begin_layout Standard
Case II: Unknown Time-invariant Parameters [9] 
\end_layout

\begin_layout Standard
For this case, since the parameter s, does not change, the memory function
 is given by a = s , with an initial probability given by p.
 = Pr[s = S.], i = 1,2, ...
 N The dynamics and measurement equations are 
\end_layout

\begin_layout Standard
y k = ^ (a k } y k-l + r(a k } w k-l 
\end_layout

\begin_layout Standard
2 k = H(a k } y k + n k* 
\end_layout

\begin_layout Standard
Again the posterior path probabilities for s are given by equation (26).
 The likelihoods are determined from equation (29), but since there is no
 path branching, the Kalman filters all operate in parallel, each on a different
 conditioning S-.
 
\end_layout

\begin_layout Standard
84 
\end_layout

\begin_layout Standard
Additionally, since the parameter transition probabili- ties (k ^_ 1) are
 given by Pr [s, = S.
 | s, _, = S.] = 6, (i-j) , the sum over the previous paths, nmq, in equation
 (26) becomes a single term for each path extension, and (26) reduces to
 
\end_layout

\begin_layout Standard
p'^'tS.U* P < S i> = 5-T-.
 
\backslash
 ; A - l ' 2 ••• N 
\end_layout

\begin_layout Standard
j-1 ] 3 
\end_layout

\begin_layout Standard
which is Lainiotis 1 result [9].
 Note that since there is no branching of the paths, the exact optimum solution
 for this case is realizable.
 
\end_layout

\begin_layout Section
A PRACTICAL HKM MODEL 
\end_layout

\begin_layout Standard
While the results of the preceding theoretical develop- ment show how optimum
 estimation of the state of the HKM process may be performed, it remains,
 of course, to specify the parameters of the model.
 In this section, specific values for the model parameters are derived and
 it is shown in principle how increasingly complex models may be obtained.
 While the specific model derived in this section is one which considers
 the letters of the text to be independent and equally likely, it is shown
 in principle how this model may be easily extended to include contextual
 message information as well.
 
\end_layout

\begin_layout Standard
The parameters to be determined are given by equations (9) : 
\end_layout

\begin_layout Standard
P (s k u kK-l 
\backslash
-l ] and f a (s k Vl } ' 
\end_layout

\begin_layout Standard
that is, the state probability transition matrix and the recursive memory
 function.
 These expressions are given in terms of the components of s, , u, , a,
 by equations 9a and 9b: 
\end_layout

\begin_layout Standard
Keystate transition matrix: p(x,|a, u 8, , a, , ) 
\end_layout

\begin_layout Standard
86 
\end_layout

\begin_layout Standard
Morse symbol transition matrix: p (a, 
\backslash
l u a, A g .) 
\end_layout

\begin_layout Standard
Text Letter transition matrix: p(£ |X a v-i ^ 
\end_layout

\begin_layout Standard
Control transition matrix: p (u, |q, 1 a, , 3 v _-i A, .
 ) 
\end_layout

\begin_layout Standard
Keystate memory function: f (x, ,$.
 -, ) 
\end_layout

\begin_layout Standard
Morse Encoder memory function: f (a, , a, n ) 
\end_layout

\begin_layout Standard
TEXT memory function: f (£ ,, A, , ) 
\end_layout

\begin_layout Standard
Thus the problem is to determine reasonable values for the probability assignmen
ts (9a) and to construct the recursive functions (9b) which account for
 the portion of the process which can be described deterministically .
 
\end_layout

\begin_layout Subsection
KEYSTATE MODEL 
\end_layout

\begin_layout Standard
The simplest usable model of the evolution of the keystate would be the
 simple Markov model described by: 
\end_layout

\begin_layout Standard
P(x kl x k-1 ) = Pr [x k = ^ l x k-l =i] 7 i '^ = °' 1 
\end_layout

\begin_layout Standard
This model suppresses any dependence of the transition probability on current
 and past Morse symbols ( a %./Ct,_i ) and speed of transmission (u, ) ,
 and limits the dependence on past history of the keystate to the immediate
 past, x, .
 Such a model would have the memory function: 
\end_layout

\begin_layout Standard
87 
\end_layout

\begin_layout Standard
s k " WW 
\end_layout

\begin_layout Standard
x.
 
\end_layout

\begin_layout Standard
The four Markov transition probabilities Pr [x, =l| x, _, =1] , Pr [x k
 =l| x k _ 1 =0] , Pr [x k =0| x k-1 =0] , Pr [x k =0| x k _ 1 =l] can be
 obtained empirically by determining the relative frequency of the states
 11, 10 , 00, 01 in a large ensemble of actual hand-keyed Morse messages.
 Clearly these probabilities are dependent on the sampling rate.
 As a simple example, consider the possible realization of an HKM sequence
 as illustrated in Figure 11, with the resulting transition probabilities
 for this sequence given in Table VIII.
 
\end_layout

\begin_layout Standard
Figure 11.
 Example Of Sampled HKM Process 
\end_layout

\begin_layout Standard
TABLE VIII Transition Probabilities For Illustrative HKM Process 
\end_layout

\begin_layout Standard
If the sample rate were different from that illustrated then obviously the
 relative frequency of each of the transitions would be different; this
 dependence on sample rate is shown in Table IX.
 
\end_layout

\begin_layout Standard
TABLE IX Transition Probability As Function Of Sample Rate 
\end_layout

\begin_layout Standard
This artificially induced dependence of the keystate transition probability
 on sample rate is undesirable from a modeling viewpoint since, in reality,
 the continuous-time HKM process generated by the sending operator has no
 such dependence, and it is intuitively unsatisfactory to require the statistics
 of the sending operator to fit an arbitrarily selected time scale.
 
\end_layout

\begin_layout Standard
This dependence can be removed by normalizing the time- scale to the element-dur
ation, whereby instead of measuring the sample rate in samples per second,
 the sample rate is measured in samples per duration in elements.
 Consider, then, the following expressions for describing the keystate evolution
: 
\end_layout

\begin_layout Standard
p(x J u k 6 k-l } 
\end_layout

\begin_layout Standard
Pr[x k = ^ u k =U i' e k-l =B n ] 
\end_layout

\begin_layout Standard
k-1 
\end_layout

\begin_layout Standard
x, 
\end_layout

\begin_layout Standard
*k = Vl (1 -
\backslash
- X k-l + 2x k x k-l )+1 
\end_layout

\begin_layout Standard
where it is seen that the recursion for d>, counts the number of samples
 since the last zero-one or one-zero keystate transition.
 This description then conditions the keystate transition probabilities
 not only on the immediate past keystate x, ,, but also on the data rate
 u, , and the number of samples, <£, / that the key has been in a 1 or state
 since the last transition.
 
\end_layout

\begin_layout Standard
Now if <£, is given in samples with a sampling interval t, then T, = <{>,
 t is the amount of time (in seconds) since the last to 1 or 1 to transition.
 If i% is given in terms of words-per-minute, then the element duration
 for this rate is r, = (6/5) x (1/u, ) .
 Thus the normalized time for this data rate is given by: 
\end_layout

\begin_layout Standard
T* = T,/r.
 
\end_layout

\begin_layout Standard
k k k 
\end_layout

\begin_layout Standard
5 *k u k T 
\end_layout

\begin_layout Standard
90 
\end_layout

\begin_layout Standard
This description of the keystate transition probabilities is clearly more
 satisfying since it depends only on the individual sending operator's rate
 of transmission and keying characteristics, and not on the sample rate.
 
\end_layout

\begin_layout Standard
The model is still not complete, however, since it does not allow for dependence
 on the type of Morse symbol being keyed, clearly for dots and element spaces,
 transitions between mark and space states occur more frequently than for
 dashes, character spaces, word spaces, and pauses.
 Additionally, these transition probabilities depend to some extent on the
 previously keyed symbols, with the degree of dependence being a function
 of the type of key used.
 For mechanical bugs, a series of dots separated by element spaces is sent
 by simply holding the paddle in one position, creating a string of symbols
 with virtually equal durations.
 When sending a dot/dash combination, however, the element space duration
 is determined by the operator's dexterity and not by a mechanical device,
 so the variability of this ele- ment space duration is higher than that
 for the repeated dot sequence.
 A similar effect occurs when the key is an elec- tronic bug, although the
 variability of repeated symbols is even less than that for the mechanical
 bug.
 The same type of dependence on past symbols has been noted even for senders
 using a telegraph key [12] [13] .
 It has been found that the primary effect is that of reduced variability
 of element-space durations when the preceeding symbol was a 
\end_layout

\begin_layout Standard
91 
\end_layout

\begin_layout Standard
dot (a detailed analysis of the effect of key type on keystate statistics
 may be found in [13]).
 
\end_layout

\begin_layout Standard
While the keystate transition probabilities have been noted to be dependent
 on the preceeding symbol sequence, this dependence is clearly a second-order
 effect when con- ditioned on the current symbol.
 In the model developed here, then, these second-order effects are ignored
 and the final expressions for the keystate transition probability model
 are given by: 
\end_layout

\begin_layout Standard
P (x J a k u k 8 k-l> = PrCx k^l a k" A i'V U m' B k-l" B n ] 
\end_layout

\begin_layout Standard
S k = 
\end_layout

\begin_layout Standard
k x k 
\end_layout

\begin_layout Standard
*k = *k-l (1 - x k- x k-l + 2x k x k-l } + !• 
\end_layout

\begin_layout Standard
In terms of the normalized time scaled, the transition 
\end_layout

\begin_layout Standard
probabilities are Pr [x,= j | x k _]_ =i ' a k =A n' r k' T k-l^ * For example,
 the probability Pr [x, =1 |x, , =1 ,a =dot,r, =r, ,T'_ = t] 
\end_layout

\begin_layout Standard
is the probability that at time k, the key will remain in 
\end_layout

\begin_layout Standard
state 1, given that the operator is sending a dot, that his 
\end_layout

\begin_layout Standard
average element duration is r, , and that they key has been 
\end_layout

\begin_layout Standard
in state 1 for t element durations.
 Clearly if t is close 
\end_layout

\begin_layout Standard
to zero, then this probability is nearly 1; and similarly 
\end_layout

\begin_layout Standard
if t > 2, then the probability is small.
 
\end_layout

\begin_layout Standard
An equivalent expression of this probability is the 
\end_layout

\begin_layout Standard
probability that the duration T'_ , becomes duration 
\end_layout

\begin_layout Standard
92 
\end_layout

\begin_layout Standard
T,* = TJ , + T/r, since if x, =1, then t<J>.
 = t d>.
 , + t  k k-1 k k T k T k-1 
\end_layout

\begin_layout Standard
T, , + x.
 This probability can be determined from the den- sity of symbol durations,
 conditioned on speed r, and symbol type.
 
\end_layout

\begin_layout Standard
The modeling of the symbol duration densities has been a topic of considerable
 interest among investigators working on the Morse decoding problem.
 In the past, because of lack of sufficient empirical data, these densities
 have been assumed to be truncated gaussian or uniform [2] [14].
 A recent intensive modeling investigation by Technology Services Corporation
 [13] , did indeed demonstrate the not surprising result that when normalized
 for speed variation, the density of each symbol duration, averaged over
 several operators, approaches the gaussian density.
 For individual operators, however, the densities are far from gaussian,
 and no single normalizing technique was found which would allow for para-
 metric estimation of the individual densities.
 Thus, the problem of parameterizing the symbol duration densities of individual
 Morse operators remains open.
 Indeed, the evidence supported by the data accumulated so far indicates
 that estimation of these highly individualistic densities must be accomplished
 on-line using a combination of parametric and non-parametric techniques.
 
\end_layout

\begin_layout Standard
It is not the purpose of the present research to delve, yet again, into
 this density estimation problem, but to show, whatever, the proper density,
 how it can be used most effec- tively for Morse transcription.
 For the purposes of the HKM model developed here, then, a parametric symbol
 duration density is hypothesized and justified on the basis of intui- tive
 arguments.
 Traditionally, the local speed of the Morse signal in wpm is defined as
 1.2 times the reciprocal of the element duration (in sec) , averaged over
 10-20 mark-space pairs.
 A histogram of the normalized symbol duration (actual duration in seconds
 divided by average element duration) is then taken to be an estimate of
 the shape of the density function for that symbol.
 The new approach to be considered here is to hypothesize an instantaneous
 speed of transmission, defined to be the speed at which a single symbol
 is sent.
 The instantaneous element duration (baud) is likewise defined on an individual
 symbol basis.
 The effect produced by assigning appropriate probability densities to each
 results in the same description for an average 10-20 mark-space pair segment
 as does the traditional approach.
 The reason for hypothesizing such parameters is simply because it is more
 intuitively satisfying to propose the existence of individual symbol statistics
 whose average behavior duplicates the observed empirical behavior, rather
 than to propose that the statistics of each individual symbol are identical
 to the observed average statistics.
 Although this distinction is a fine point, it allows greater flexibility
 in estimating the keystate transition probability with fewer parameters.
 
\end_layout

\begin_layout Standard
Consider then the following hypothesized random variables : 
\end_layout

\begin_layout Standard
94 
\end_layout

\begin_layout Standard
r = instantaneous speed of transmission A = instantantous element duration
 (baud) 
\end_layout

\begin_layout Standard
and let dot and element-spaces have duration = A; dashes and character spaces
 = 3 A; word-space = 7A; pause = 14 A.
 
\end_layout

\begin_layout Standard
Then in terms of the actual symbol duration, d : 
\end_layout

\begin_layout Standard
2 m 
\end_layout

\begin_layout Standard
a d A A -S , 
\end_layout

\begin_layout Standard
m 
\end_layout

\begin_layout Standard
where m = 1, 3, 7 , 14 as appropriate.
 
\end_layout

\begin_layout Standard
The normalized symbol duration, in terms of A and r is given by: 
\end_layout

\begin_layout Standard
*4 = <!> Ar 
\end_layout

\begin_layout Standard
Note that while A is well-defined in terms of a measurable quantity, r is
 arbitrary.
 However, it is convenient to define r such that its value is indicative
 of the actual speed: 
\end_layout

\begin_layout Standard
A ,6, 1 
\end_layout

\begin_layout Standard
r mean ' K 5 } A 
\end_layout

\begin_layout Standard
Although this expression determines the statistical behavior 
\end_layout

\begin_layout Standard
of r through its dependence on the random variable A, mean 3 c 
\end_layout

\begin_layout Standard
clearly it does not restrict the freedom to assign appropriate 
\end_layout

\begin_layout Standard
95 
\end_layout

\begin_layout Standard
statistical description to the other moments of the random 
\end_layout

\begin_layout Standard
variable r, independent of the statistics of A.
 
\end_layout

\begin_layout Standard
Consider now the random variable & .
 , and note that mcji .
 
\end_layout

\begin_layout Standard
r A A 
\end_layout

\begin_layout Standard
is the normalized symbol duration (in elements) , given that the symbol
 was transmitted at rate r.
 A density for m<£ A , conditioned on r, then describes the keystate duration
 random variable, normalized for speed.
 Let this random variable be described by the Laplacian density (double-sided
 exponential) with mode m and parameter a, as illustrated in Figure 12,
 below.
 
\end_layout

\begin_layout Standard
c - 
\end_layout

\begin_layout Standard
p(m<f> A /r) 
\end_layout

\begin_layout Standard
m ^A (-5/6 mAr) 
\end_layout

\begin_layout Standard
Figure 12.
 Laplacian Duration Densities 
\end_layout

\begin_layout Standard
96 
\end_layout

\begin_layout Standard
In terms of the speed r: 
\end_layout

\begin_layout Standard
^ a(5/6 mAr - m) , 
\end_layout

\begin_layout Standard
ce ; mcj) .
 <_ m 
\end_layout

\begin_layout Standard
p(m<J> A /r) = 
\end_layout

\begin_layout Standard
a (m - 5/6 mAr) , ce ; mcj) _> m 
\end_layout

\begin_layout Standard
The parameter a and coefficient c are to be chosen such that Pr[l(|> A >
 2] = Pr[3<J> A <_ 2] = .0135; that is, the probability of error in sending
 a dot for a dash or an element space for a character space (and vice versa)
 is arbitrarily selected to be 1.35%.
 This symbol error rate was found to be the average error using optimum
 separation thresholds for 55 samples of hand-keyed Morse studied in the
 TSC analysis [13] ; and since the densities are conditioned on the instan-
 taneous speed, the normalized optimum threshold is halfway between m =
 1 and m = 3.
 On this basis, then, a and c are determined as follows: 
\end_layout

\begin_layout Standard
Pr[16 A > 2] = / p( 14> A /r) d4> A 
\end_layout

\begin_layout Standard
ad - 4> A ) ce d <j) A 
\end_layout

\begin_layout Standard
2 = c/a e 
\end_layout

\begin_layout Standard
Likewise : 
\end_layout

\begin_layout Standard
97 
\end_layout

\begin_layout Standard
Pr[3(J) A £ 2] = c/a e~ a 
\end_layout

\begin_layout Standard
The probability density requirement gives the other equation needed: 
\end_layout

\begin_layout Standard
/ 
\end_layout

\begin_layout Standard
p(m<J> A /r) d<J> A 
\end_layout

\begin_layout Standard
1 a(cf> - 1) f a(l-«J> A ) 
\end_layout

\begin_layout Standard
ce a d(j» A + ce n d<f> A = 1 
\end_layout

\begin_layout Standard
1 
\end_layout

\begin_layout Standard
/ 
\end_layout

\begin_layout Standard
c/a + c/a = 1 
\end_layout

\begin_layout Standard
c = a/2 
\end_layout

\begin_layout Standard
Solving for a, c gives, for dots, dashes, element spaces, character spaces
 : 
\end_layout

\begin_layout Standard
a = 3.61 c = 1.81 
\end_layout

\begin_layout Standard
Using the same procedure for word space (m=7) and pause (m=14) , the values
 for the densities are: 
\end_layout

\begin_layout Standard
word spaces: a = 1.81, c = .90 pause: a = .90, c = .45 
\end_layout

\begin_layout Standard
98 
\end_layout

\begin_layout Standard
Having constructed the duration densities, the speed- conditioned keystate
 transition probabilities can now be determined.
 
\end_layout

\begin_layout Standard
Let D be the current normalized keystate duration, i.e., the amount of time
 (in terms of instantaneous element duration) since the last to 1 or 1 to
 transition.
 Then the required probabilities are Pr[<f>.
 >_ D + e/x, _, ,a.
 ,r, ,<{>.
 >_ D ] , where £ is the normalized sampling interval given by £ = t/A.
 It is seen that this expression gives the transition probabilities in terms
 of the probability of extending dura- tion D for one more sample interval.
 The conditioning parameters provide the normalization coefficients to be
 used for p(m<|) A /r).
 Given the appropriately scaled density then, 
\end_layout

\begin_layout Standard
Pr[<j> D +e;<J> A D ] 
\end_layout

\begin_layout Standard
Pr[cK > D +e/4> A > D ] = g , , ° n S — 
\end_layout

\begin_layout Standard
Y A — o Y A — o Pr[6 A > D J 
\end_layout

\begin_layout Standard
A - o 
\end_layout

\begin_layout Standard
but £ > 0, so D +e > D , and the joint probability becomes 
\end_layout

\begin_layout Standard
Pr[<f> A > D Q +£;c{) A > D Q ] = Pr [<
\backslash
> A > D q +£] , 
\end_layout

\begin_layout Standard
and so the conditional probability is given by: 
\end_layout

\begin_layout Standard
Pr ^A - D o +£] Pr[<f>.
 D +e/<f>.
 > D] 
\end_layout

\begin_layout Standard
A - o ' y A - o J Pr[<|) A > D ] 
\end_layout

\begin_layout Standard
Y A — o 
\end_layout

\begin_layout Standard
where Pr[<J>.
 _> D ] , Pr[<t> A D +e] are computed as follows 
\end_layout

\begin_layout Standard
99 
\end_layout

\begin_layout Standard
D +£ 
\end_layout

\begin_layout Standard
o 
\end_layout

\begin_layout Standard
, -a(D +e-m) 
\end_layout

\begin_layout Standard
h ° 
\end_layout

\begin_layout Standard
IT 
\end_layout

\begin_layout Standard
D +£ > m o — 
\end_layout

\begin_layout Standard
, a(D +e-m) 
\end_layout

\begin_layout Standard
1 - T^e ; D +e < m 
\end_layout

\begin_layout Standard
2 o — 
\end_layout

\begin_layout Standard
Similarly: 
\end_layout

\begin_layout Standard
Pr[* a > D o ] = J p(* A ) d+ A 
\end_layout

\begin_layout Standard
o 
\end_layout

\begin_layout Standard
o 
\end_layout

\begin_layout Standard
/ 
\end_layout

\begin_layout Standard
, -a(D -m) 
\end_layout

\begin_layout Standard
ie ° 2^ 
\end_layout

\begin_layout Standard
D > m o — 
\end_layout

\begin_layout Standard
, a(D -m) 
\end_layout

\begin_layout Standard
; D < m o — 
\end_layout

\begin_layout Standard
Forming the quotient of these probabilities in the appro- priate ranges
 gives: 
\end_layout

\begin_layout Standard
Prt* A ^D o+ e/* A > d ] 
\end_layout

\begin_layout Standard
-ae 
\end_layout

\begin_layout Standard
, a(D +e-m) 
\end_layout

\begin_layout Standard
x 2^ 
\end_layout

\begin_layout Standard
1 -a (D -m) 
\end_layout

\begin_layout Standard
, D > m o — 
\end_layout

\begin_layout Standard
D < m o — 
\end_layout

\begin_layout Standard
D + £ > m o — 
\end_layout

\begin_layout Standard
, a (D +e-m) 
\end_layout

\begin_layout Standard
1 2^ 
\end_layout

\begin_layout Standard
" a(D -m) 1-^e ° 
\end_layout

\begin_layout Standard
, D +e < m o — 
\end_layout

\begin_layout Standard
100 
\end_layout

\begin_layout Standard
The above expression then represents the keystate transition probability
 for the "transitions" 1-1 and 0-0, conditional on the current symbol type,
 data rate, and length of time already in state 1 or .
 The probabilities for the transi- tions 1-0 and 0-1 are found, obviously,
 by subtracting from 1.
 
\end_layout

\begin_layout Subsection
SPEED TRANSITION MODEL 
\end_layout

\begin_layout Standard
The random control vector u may contain components which model operator
 sending peculiarities such as random insertions of extra dots, slurs, character
 splitting, or any other feature of interest which controls the manner in
 which encoding takes place; it is not limited to speed con- trol alone.
 However, the peculiarities mentioned above are highly individualistic and
 little modeling of these peculiarities has been done.
 It is conjectured that such modeling will have the same fate as that of
 attempting to obtain a general parametric model of the keystate duration
 densities; that is, no general model will be found, and such modeling will
 require on-line estimation techniques.
 For the purposes of the HKM model developed here, these peculiarities are
 ignored, and the only component of the control vector u.
 considered is the instantaneous speed r.
 
\end_layout

\begin_layout Standard
k 
\end_layout

\begin_layout Standard
The speed transition probabilities are developed on an intuitive basis seasoned
 with experience and the results of the TSC study on observed hand-sent
 code speed variability In that study it was found that, on the average,
 hand-sent code exhibits a speed difference of about 2.5 wpm between segments
 of 10 mark-space pairs, but that it is not uncommon to observe a speed
 difference of 8-10 wpm between segments.
 
\end_layout

\begin_layout Standard
Now observing that the speed transition probability expression of the HKM
 model, p(u, |u,_-, a 1< ._-i 3,-, ^i ) / allows for conditioning on the
 entire past history of the state of the HKM process, it can be seen that
 this transition probability may take into account such items as message
 duration (for modeling the effect of operator fatigue) , the actual text
 itself (for modeling the effect of speed changes due to sending different
 types of text material) , or any other feature which may have an effect
 on sending speed.
 The only conditioning to be considered here, however, is the immediate
 past speed u, ,, the past history of the encoded output, 
\end_layout

\begin_layout Standard
a.
 , , and the keystate duration 6, , .
 Let k-1 J k-1 
\end_layout

\begin_layout Standard
R.
 e {i; 10 i 60, i an integer}; that is, a set of discrete speeds in wpm
 between 10 and 60 wpm.
 The following model for p(u, |u, ,;•) is proposed: 
\end_layout

\begin_layout Standard
If 3-^1 7* (no change in keystate) , then 
\end_layout

\begin_layout Standard
P (U kl U k-l a k-l e k-l } = Pr[u k = R i |u k-1 = R j' a k-l' 3 k-l *
 ° 
\end_layout

\begin_layout Standard
0, if i ji j 
\end_layout

\begin_layout Standard
1, if i = J 
\end_layout

\begin_layout Standard
102 
\end_layout

\begin_layout Standard
That is, the speed is not allowed to change except when the keystate changes
 from to 1 or 1 to , no matter what the previous symbol is.
 For 8,-, = 0, the speed transition probabilities are made conditional on
 the type of Morse symbol just completed: 
\end_layout

\begin_layout Standard
For a., -*indicates dot, dash, e-sp: 
\end_layout

\begin_layout Standard
Pr[u k = Rj ± 2i,lu k = K j ,a k .
 1 .8 k _ 1 = 0] = p..
 (a^) 
\end_layout

\begin_layout Standard
where i = 0, 1, 2.
 
\end_layout

\begin_layout Standard
This assignment of tansition probabilities allows the speed to change by
 increments of 0, ±2, ±4 wpm according to the probability p.
 .
 (a, _, ) .
 
\end_layout

\begin_layout Standard
For a, _, -> indicates c-sp, then the increment remains 
\end_layout

\begin_layout Standard
the same, but the transition probability assignments may 
\end_layout

\begin_layout Standard
be different.
 
\end_layout

\begin_layout Standard
For a, , •* indicates word-sp, the increment is increased k-1 
\end_layout

\begin_layout Standard
to 5, and for a, , > indicates pause, the increment is 10.
 
\end_layout

\begin_layout Standard
k-1 
\end_layout

\begin_layout Standard
To complete the model, the p..
 (a, .
 ) remain to be selected.
 These probabilities, which were selected on the basis of speed differences
 reported by TSC (and on intuitive appeal) , are given in Table X.
 
\end_layout

\begin_layout Standard
Note that the absolute average speed differences for the four categories
 correspond roughly to the ranges observed by TSC.
 
\end_layout

\begin_layout Standard
103 
\end_layout

\begin_layout Standard
TABLE X Symbol-Conditional Speed Transition Probabilities 
\end_layout

\begin_layout Standard
Symbol Just Speed Increment/Probability Average Completed (wpm) Increment
 (wpm) 
\end_layout

\begin_layout Standard
dot, dash, e-sp -4-2024 1.6 
\end_layout

\begin_layout Standard
.1 .2 .4 .2 .1 
\end_layout

\begin_layout Standard
c-sp -4-2024 2.0 
\end_layout

\begin_layout Standard
.15 .2 .3 .2 .15 
\end_layout

\begin_layout Standard
w-sp -10 -5 5 10 4.0 
\end_layout

\begin_layout Standard
.1 .2 .4 .2 .1 
\end_layout

\begin_layout Standard
pause -20 -10 10 20 10.0 
\end_layout

\begin_layout Standard
.15 .2 .3 .2 .15 
\end_layout

\begin_layout Subsection
MORSE SYMBOL TRANSITION MODEL 
\end_layout

\begin_layout Standard
The symbol transition probabilities, conditional on the letter being sent,
 are obviously either zero or 1, since knowing the letter specifies the
 code sequence.
 If the model is only a first or second-order Markov model, then the symbol
 transition probabilities for various types of text may be computed.
 Since it is desired to test the performance of the estimator as a function
 of modeling complexity, these probabilities were estimated for both a first
 and second order model and are given in Tables XI and XII, respectively.
 
\end_layout

\begin_layout Standard
104 
\end_layout

\begin_layout Standard
TABLE XI First-Order Markov Symbol Transition Matrix 
\end_layout

\begin_layout Standard
.5 
\end_layout

\begin_layout Standard
TABLE XII Second-Order Markov Symbol Transition Matrix 
\end_layout

\begin_layout Standard
105 
\end_layout

\begin_layout Standard
The encoder memory function, f , may be constructed to record the previous
 symbol for the first-order model, or the previous two symbols in the second-ord
er case.
 In case the symbol transition probability is made conditional on the letter
 being sent, there is no need to record previous symbols for use by the
 encoder.
 As a minimum, however, the function f must record the previous symbol for
 use by the speed transition probability, since it has been made conditional
 on this symbol.
 
\end_layout

\begin_layout Subsection
TEXT LETTER TRANSITION MODEL 
\end_layout

\begin_layout Standard
For equally likely independent letters, the letter transition probabilities
 are uniform, and the only con- ditioning necessary is on a, - so that when
 a, , indicates the end of a letter, the letter transition is allowed to
 occur.
 During the period when a.
 , does not contain a c-sp, w-sp, or pause, obviously the letter transition
 probability is zero.
 This case of equally likely letters is the highest complexity modeling
 actually coded and tested in this investigation.
 It is clear from the theoretical error-rate analysis of section III, however,
 that the largest payoff in terms of increase performance is to be found
 in more sophisticated models for this transition probability and memory
 function.
 This fact was recognized early by Gold [12] in his study of the Morse decoding
 problem, in which he developed the MAUDE algorithm for decoding of the
 demodulated Morse waveform: "The conclusion is inescapable, therefore,
 that for the automatic reception of a language encoded by even a simple
 process like Morse code, a machine must have some knowledge of the language
 if it is to approximate the performance of a man." 
\end_layout

\begin_layout Standard
The major difficulty, however, in modeling the message text is that the
 type of text is not constant.
 The letter dependencies are highly variable among such traffic types as
 call-up, response, chatter, formatted messages, plain language messages,
 code groups, etc.
 Here again, then, it is conjectured that the only real solution is to perform
 on-line modeling of this transition probability and memory function.
 Clearly a straightforward application of probability estimation techniques,
 while feasible, is simply not practical in this case.
 For a third-order model, the 4 storage requirements would be on order of
 36 = 1,679,616 words, just to store the transition probability matrix.
 
\end_layout

\begin_layout Standard
The f function would require 36 locations to keep track of the three prior
 letters.
 Although some reduction in memory could beaccomplished since some letter
 combination rarely occur, it is evident that the storage requirement is
 large.
 The most promising technique for utilizing the decrease in source entropy
 may be one similar to that for recognition of speech using a linguistic
 statistical decoder [15] , with appropriately modeled linguistic elements
 and using an appropriate channel model [16] .
 If a suitably flexible grammar for a set of Morse messages can be defined
 then perhaps a form of syntactic decoding is in order [17] .
 If the semantics of the message are well-understood then one possible approach
 is to use a dictionary look-up to form the f function, on a word basis.
 This technique for English text messages is under investigation by an ARPA-
 funded MIT project, but a final report of the results has not yet been
 issued.
 The Army Research and Development Agency is currently studying the possibility
 of defining a grammar for a specified set of Morse messages for use in
 syntactic decoding.
 These kinds of techniques for dynamic on-line construction of the f function
 and estimation of the transition probabilities are clearly the only realistic
 methods of reducing the entropy of the text sufficiently to obtain error
 rates comparable to that of the human operator, in any situation except
 for random letter groups.
 
\end_layout

\begin_layout Section
A PRACTICAL HKM CHANNEL MODEL 
\end_layout

\begin_layout Standard
The general baseband HKM channel model developed in Section IV is given
 by the channel and observation equations (10) : 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{k=\gamma F\left(s_{k}\sigma_{k-1}\right)y_{k-1}+P\left(s_{k}\sigma_{k-1}\right)w_{k}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
z_{k}=H(s_{k})y_{k}+n_{k}
\]

\end_inset

where z, is the sampled output of the detector.
 The specific model to be considered here requires the parameter y and functions
 F, r, H, to be selected such that the resulting model has the following
 features: 
\end_layout

\begin_layout Enumerate
The noise process represented by 
\begin_inset Formula $n_{k}$
\end_inset

, is a zero-mean white gaussian process, with known variance 
\begin_inset Formula $R_{k}$
\end_inset

 .
 
\end_layout

\begin_layout Enumerate
The amplitude 
\begin_inset Formula $y_{k}$
\end_inset

, is observed only when 
\begin_inset Formula $x_{k}=1$
\end_inset

 , that is, during the signal on-time (MARK), so that 
\begin_inset Formula $H\left(s_{k}\right)=H\left(x_{k}\right)\equiv x_{k}$
\end_inset

.
\end_layout

\begin_layout Enumerate
During a MARK, the fading amplitude process obeys a linear gauss Markov
 process given by: 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $y_{_{k}=\gamma y_{k-1}+v_{k}}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 , where the parameter y and the variance of v, are selected to represent
 the fading observed at the detector output.
 
\end_layout

\begin_layout Enumerate
The observed effective transmitted amplitude is a random variable which
 obeys the following time- varying linear gauss-Markov process: 
\begin_inset Formula $y_{k}=$
\end_inset

 F(x k a k Wk-l + r(x k a k S k-l )w ! , where F and F are selected such
 that: (a) During a MARK the transmitted amplitude remains constant.
 (b) During a space the amplitude can change, the amount of change being
 dependent on the type and duration of the space.
 
\end_layout

\begin_layout Enumerate
It is assumed that the detected signal has been gain-leveled by an AGC,
 so that the average detected output power is normalized.
 The parameter selection and function construction process for each of these
 features is discussed below.
 
\end_layout

\begin_layout Subsection
THE OBSERVED NOISE PROCESS 
\end_layout

\begin_layout Standard
Since the noise process observed at the output of the detector is the result
 of envelope detection of a narrowband gaussian process, the resulting process
 is neither zero-mean, gaussian, nor white.
 The sampled process, however, has independent noise values if the sample
 interval 
\begin_inset Formula $\tau$
\end_inset

 satisfies 
\begin_inset Formula $\tau\geq1/2B_{BPF}$
\end_inset

, where 
\begin_inset Formula $B_{BPF}$
\end_inset

is the bandwidth (in Hz) of the band-pass filter preceding the envelope
 detector, provided that also the bandwidth of the low-pass filter of the
 envelope detector is greater than 
\begin_inset Formula $2B_{BPF}$
\end_inset

.
 If x is less than this value, then the sampled noise is correlated, and
 a model which accounts for this correlation would theoretically provide
 for better estimation.
 Several techniques are available for such modeling, [18 ] and should be
 used if the noise is correlated.
 Clearly if 
\begin_inset Formula $\tau$
\end_inset

 is selected purely on this basis alone, then the assumption on independence
 can be satisfied.
 There may be, however, other competing constraints on the selection of
 
\begin_inset Formula $\tau$
\end_inset

, and although the value selected may render the independent noise assumption
 invalid, its effect can be minimized by selecting it as large as possible
 within the other constraints.
 
\end_layout

\begin_layout Standard
The bandwidth of the bandpass filter is selected on the basis of the largest
 signal bandwidth expected.
 The highest code-speed under consideration for this processor design was
 selected to be 50 wpm, which has a minimum pulse duration (MARK) of 24
 msec.
 The specific filter implementation was selected to be a cascade of two
 single-tuned resonators, since this combination has a respectable ratio
 of noise bandwidth to 3-dB bandwidth of 1.22 [19] , and can be coded with
 relatively few multiplication per sample.
 For this filter implementation the optimum bandwidth as given by Skolnik
 [19] is .613/.
 024 = 25 Hz, and has only .56 dB of loss in SNR compared to the matched
 filter.
 Although such a narrow bandwidth greatly increases the SNR of a signal
 in a 4 kHz receiver bandwidth and effectively eliminates most interferers,
 it is clearly too narrow to accept signals which have a significant carrier
 instability due to chirp or drift.
 Since it is not uncommon to observe carriers with a chirp on the order
 of 50 or so Hz, the bandwidth required is on the order of 100 Hz.
 There is obviously a strong motivation, therefore, to investigate filtering
 techniques which would adapt to the chirp, since a 100 Hz wide filter represent
s a loss of 6 dB compared to the optimum bandwidth of 25 Hz.
 Motivation for adaptive filtering techniques is also provided by the fact
 that at 20 wpm the optimum bandwidth is only .613/.
 060 = 10 Hz, thus there is a 10 dB loss in SNR compared to the optimum
 bandwidth when using a 100 Hz filter.
 
\end_layout

\begin_layout Standard
For this investigation, since the primary emphasis is on optimum demodulation
 and decoding techniques, a fixed 100 Hz band-pass filter is used.
 For this bandwidth, then, the sample rate may be selected to be 200 Hz,
 with a resulting sample interval of 5 msec.
 Since this quantization is con- sidered adequate for representing the minimum
 duration 24 msec- long pulse of the 50 wpm code with sufficient precision,
 then 
\begin_inset Formula $\tau$
\end_inset

 is selected to be 5 msec, resulting in independent noise samples.
 
\end_layout

\begin_layout Standard
Since approximately 5 msec, is the largest quantization allowable for adequate
 precision in representation of the code symbols, and since adaptive techniques
 for the band- pass filter would result in narrower bandwidths , the assumption
 on independent noise samples would be violated for this case, requiring
 a model which accounts for correlated noise, if optimum techniques are
 to be pursued.
 
\end_layout

\begin_layout Standard
Although the zero-mean assumption on the output noise process is violated,
 a zero-mean process may be approximated by estimation of the mean and subtracti
on of it from the detected output.
 Estimation of this mean value also pro- vides an estimate of the noise
 variance, 
\begin_inset Formula $R_{k}$
\end_inset

, , which has been assumed to be a known value throughout.
 (Again, although techniques are available for modeling in the case of unknown
 noise intensity, the simplified approach taken here is to use the estimate
 of 
\begin_inset Formula $R_{k}$
\end_inset

, as if it were the true value.
 It can be seen in section IX, Table XIII, that the resulting processor
 is relatively insensitive to 
\begin_inset Formula $\hat{R}_{k}$
\end_inset

, as long as 
\begin_inset Formula $\hat{R}_{k}$
\end_inset

, is within a rather large range of the true value.) Estimation of the mean
 noise level relies on the following relationships 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X_{t}$
\end_inset

 be a white gaussian random process with one-sided density 
\begin_inset Formula $N_{0}$
\end_inset

, input to the BPF; let 
\begin_inset Formula $Z_{t}$
\end_inset

be the output of the envelope detector, with 
\begin_inset Formula $B_{LPF}\geq B_{BPF}$
\end_inset

 as illustrated below: 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Envelope Detection Process
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Then, from Davenport [20], 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
ugly
\]

\end_inset


\end_layout

\begin_layout Standard
Thus if 
\begin_inset Formula $\text{\mu\_n}$
\end_inset

 can be estimated in the absence of a MARK, then :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
ugly
\]

\end_inset


\end_layout

\begin_layout Standard
and the approximation to a zero-mean process is 
\begin_inset Formula $Z_{t}-\hat{\mu}_{n}$
\end_inset

 .
 Implementation of such an estimator is described in Section VIII.
 
\end_layout

\begin_layout Standard
The assumption of a gaussian process for n, is clearly violated since the
 output of the detector has a Rayleigh density in the absence of a MARK,
 and a Rician density when signal is present.
 Thus not only are the statistics not gaussian, but also they are correlated
 with the signal when a MARK is present.
 By choosing to ignore the higher-order moments of the density (greater
 than 2), the resulting estimator based on this assumption may not be optimal
 in the sense of providing as good a conditional-mean estimate as possible,
 but it will still provide the minimum-mean- squared- error estimate.
 
\end_layout

\begin_layout Subsection
THE MEASUREMENT FUNCTION 
\end_layout

\begin_layout Standard
During the period when 
\begin_inset Formula $x_{k}=0$
\end_inset

 , the transmitter is turned off and it is not possible to observe the amplitude
 which is being used to transmit the MARKS.
 Thus only- noise is observed during this period, and by ignoring the correlatio
n between signal and noise when signal is present, the measurement equation
 is simply: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
z_{k}=x_{k}y_{k}+n_{k}
\]

\end_inset


\end_layout

\begin_layout Subsection
FADING MODEL 
\end_layout

\begin_layout Standard
The effect of fading can be observed during a MARK period, with the maximum
 fade rate being determined by the band-pass filter/dectector bandwidth,
 under worst-case HF channel conditions (rapid, intense fading) .
 For typical values of fading rate on the order of 1 Hz , the fading parameter
 
\begin_inset Formula $\gamma$
\end_inset

, for a 5 msec sampling interval is given by: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\gamma=e^{-\left(0.005\right)\left(2\pi\right)\left(1\right)}=.97
\]

\end_inset


\end_layout

\begin_layout Standard
The intensity observed at the output of the gain-controlled detector can
 be approximated for the typical 1 Hz fade rate by noting that during a
 1 sec fade period the amplitude can change by about 3 dB for a typical
 receiver AGC circuit.
 The intensity for this range of change, i.e., the variance of 
\begin_inset Formula $v_{k}$
\end_inset

, is about:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Var\left(v_{k}\right)\cong\left[2/\left(1./.005\right)\right]\text{²}=\left[2/200\right]\text{²}=0.0001
\]

\end_inset


\end_layout

\begin_layout Standard
As discussed earlier, in Section IV.
 B, when no signal is present, the effect of fading is that the subsequent
 MARK appears at an amplitude which differs from the amplitude of the previous
 MARK in such a way that it appears as if the MARKS of the signal were transmitt
ed at a random amplitude.
 Because of this effect, these mark-to-mark variations are lumped together
 with the variations caused by an actual change in transmitted power.
 
\end_layout

\begin_layout Subsection
APPARENT TRANSMITTER POWER VARIATIONS 
\end_layout

\begin_layout Standard
In addition to the Mark-to-Mark amplitude variations discussed above, the
 actual transmitted power may vary.
 Usually this effect is most prominent when working with a communications
 net, since the received power of each of the transmitters on the net will
 usually be different.
 These changes usually occur after a pause (during which one net member
 has signed off and another is preparing to sign on) ; however,, it is not
 uncommon for a new net member to sign on during a time duration for a word
 space or even a character space, especially if net discipline is good.
 It is assumed that changes do not occur during an element-space or a mark.
 The following model accounts for these effects: a) For a
\end_layout

\begin_layout Standard
117 
\end_layout

\begin_layout Standard
Part (a) is just the fading model for Marks discussed above.
 Part (b) expresses the statement that no change in amplitude may occur
 during an element space.
 Part (c) states that, at the end of an element space the transmitted amplitude
 has not changed, but a variance of .01 is asso- ciated with the amplitude
 observed on this transition.
 The value .01 is obtained by considering that at the end of an element space
 transmitted at 50 wpm, the fade may have 4 decreased the amplitude to (.97)
 = .89 of its previous 2 ~ value, thus a variance of (1 - .89) = .01 is appropriate.
 Part (d) states that for any other space, while the variance associated
 with the transmitted amplitude is zero, the amplitude is assumed to decrease
 exponentially with time at the rate (.98); and Part (e) allows a subsequent
 MARK to appear with amplitude determined by a gaussian random variable
 of variance .25.
 (The construction of the T(-) function is implied by the assignment of
 variances to the various Q .
 ) w 
\end_layout

\begin_layout Section
IMPLEMENTATION OF HKM STATE ESTIMATION ALGORITHM 
\end_layout

\begin_layout Standard
The implementation of the estimator algorithm (Eqn.
 26, 30) for the signal and channel models just described is now presented.
 In the context of this model, estimation of the keystate is referred to
 as demodulation , estimation of the Morse symbol is termed decoding , and
 estimation of the text letter is called translation .
 The estimation algorithm performs joint demodulation, decoding and translation,
 i.e., these estimates are not made in a serial fashion; rather the structure
 of the code is used in an optimal way to aid in demodulation, and the structure
 of the text is used to aid in decoding.
 From this viewpoint the algorithm repre- sents a "correlator-estimator"
 [21] technique in which a sequence of all possible keystate transitions
 are hypothe- sized and correlated with the incoming signal, and the most
 likely sequence is output as the best estimate.
 From the viewpoint of coding theory, the algorithm represents a tree decoder
 in which all possible paths of the joint state evolution of the process
 are examined and extended in an optimal way.
 If the memory function were dependent on only a finite portion of the past
 history of the process (usually a good approximation) then the tree decoder
 reduces to the Viterbi decoder.
 As implemented herein, the decoder is most like the M-Path algorithm described
 by Haccoun J22J , with the path metric being the product of the likelihood
 of the received signal along the path and the transition proba- bility
 for the path extension.
 If the decoder is constrained to save only one path, then the decision-directed
 optimal linear filter investigated in [2] is obtained.
 
\end_layout

\begin_layout Standard
Proceeding now to a detailed description, the algorithm is presented in
 terms of the Fortran code used to implement it.
 Subroutine PROCES is the main calling routine which takes an input signal
 sample each 5 msec, along with an estimate of the noise power, and calls
 the appropriate rou- tines in order.
 The first routine called for each sample point is TRPROB, which computes,
 for each previously saved path ending at node J, the probability of extending
 the path to new nodes which are labeled to indicate the joint state (keystate,
 element state, letter state, data rate).
 These probabilities are computed using the model and equa- tions described
 in the previous section.
 Next, subroutine PATH labels the new path extended to each new node with:
 (1) the number of samples since the previous keystate transition along
 that path; (2) the data rate of the new node; (3) the identity of the element
 state at the new- node; (4) the identity of the letter state at the new
 node.
 These labels are obtained from the memory function f with arguments provided
 by the identity of the path being extended and the identity of the new
 node to which the path is being extended.
 Subroutine LIKHD is then called to compute the likelihood of the input
 signal sample for each transition under the hypothesis that that particular
 transition occurred,
\end_layout

\begin_layout Standard
LIKHD maintains an array of Kalman filters for computing this likelihood
 as given in Section V.A by equation (30) , and using the specific channel
 model described in the previous section.
 
\end_layout

\begin_layout Standard
Having obtained the new path identities, transition probabilities, and likelihoo
ds, the posterior probability of each new node (i.e., each path extension)
 is computed using equation (26), in subroutine PROBP.
 Next, routine SPROB computes the posterior probability of each keystate
 (0,1) and each element state, and the conditional mean estimates of the
 data rate, by summing over the appropriate nodes.
 The MAP estimate of the keystate at this point is the demodulated signal,
 and the conditional mean estimate of the keystate is the (non-linear) filtered
 version of the detected signal.
 Also the evolution of the MAP esti- mator for the element state may be
 observed at this point, and represents the decoded message with zero decoder
 delay.
 
\end_layout

\begin_layout Standard
The next function to be accomplished is the saving of paths for the next
 iteration.
 It is at this point that the estimation algorithm becomes sub-optimal,
 since it is clearly not possible to save all paths at each stage of iteration.
 A technique which yields a high probability that the correct path will
 always be saved obviously pro- vides the best sub-optimal performance.
 Several techniques for selecting the paths to save are available.
 The simplest idea is to always save a fixed number, say M .
 It was determined empirically, however, that, while this technique does
 indeed give a high probability of saving the correct path, most of the
 time the posterior probabilities of many of the saved paths were very low
 and need not be extended at all.
 At the instant of a keystate transition, however, the probabilities become
 more uniform and it is necessary to save all the M paths.
 The next max c technique then was to save only enough paths such that the
 total probability saved was equal to P , subject to the constraint that
 M is not exceeded.
 Another technique max ^ suggested by [2 2] is to make the number of paths
 saved a function of the probability of the highest probability path, such
 that when the highest probability path has a very high probability, fewer
 paths are saved.
 Either of the last two techniques has the attractive feature that the decoding
 computational burden is adaptive to the signal-to-noise ratio and the data
 rate, and the first of these was selected for use, with the additional
 constraint that at least one path for each element state is always saved.
 This algorithm is coded in subroutine SAVEP.
 
\end_layout

\begin_layout Standard
Also in subroutine SAVEP, the saved paths and their identities are renumbered
 in order of decreasing probability and a pointer array is maintained to
 identify the previous node from which the saved path was extended.
 Additionally, the parameters of the Kalman filters are reindexed to be
 consistent with the new path indices.
 After action by SAVEP, then, the arrays are ready for the next iteration.
 
\end_layout

\begin_layout Standard
Before proceeding to the next iteration, however, the trellis of saved paths
 is updated with the new saved nodes and connected to the proper previously
 saved paths by using the pointer array.
 Decoding and translation are accom- plished within subroutine TRELIS by
 operating on the trellis of saved paths.
 Decoding is done by finding the one node, at sufficient delay, from which
 all successor paths origin- ate.
 If no such single node exists within the trellis for a maximum delay of
 200 samples (1 second delay) then decoding is obtained by reading the node
 at delay 200 which is connected to the current highest probability path,
 and all other paths not originating from this node are deleted from the
 trellis.
 Since the text has been modeled by a source of equiprobable, independent
 letters, translation is done by a simple mapping of the decoded Morse symbols
 into the proper letters and numerals.
 
\end_layout

\begin_layout Standard
There are three auxiliary processing routines for pre- processing of the
 signal, intended to simulate the operation of a receiver, bandpass filter
 and envelope detector, along with the routine to estimate the noise power
 in the detected signal and provide a zero-mean noise process.
 Subroutine RCVR converts the incoming signal at carrier frequency co to
 a frequency of 1000 Hz using an 8 kHz sample rate, and provides a single-pole
 500 Hz BW band-pass filter.
 Sub- routine BPFDET implements the 100 Hz bandwidth band-pass filter by
 a series of two digital resonators centered at 1000 Hz, and accomplishes
 envelope detection.
 The low pass filter of the envelope detector is a 100 Hz bandwidth 3- pole
 Chebyshev filter.
 Subroutine NOISE estimates the noise power present during a space condition
 by obtaining the minimum value of the envelope detected signal over a period
 of 240 samples (1.2 seconds).
 This minimum value is ob- tained at each 5-msec sample point and averaged.
 The average is then scaled, with the scale parameter selected empirically,
 to provide the estimate of y , the mean value of the envelope detected
 output during a space.
 This esti- mate is subtracted from the envelope detector output to provide
 an approximation to a zero-mean noise process; RN, 
\end_layout

\begin_layout Standard
the estimate of noise power in the detected output is then 
\end_layout

\begin_layout Standard
- 2 given by 2y 
\end_layout

\begin_layout Standard
124 
\end_layout

\begin_layout Section
IX.
 SIMULATION RESULTS 
\end_layout

\begin_layout Standard
The Fortran coded algorithm just described has been programmed on a PDP-10
 time sharing system, along with a signal simulation routine to generate
 a Morse code message, a routine to simulate transmitter effects, and a
 channel model routine.
 The text generation routine selects letters and numerals either at random
 or from a pre-defined text file.
 The corresponding Morse code sequences are generated by a table look-up,
 and the durations of each element are randomized according to a selectable
 probability law.
 (For the results presented here, the probability law used was a truncated
 gaussian such that no element is ever less than 16 msec or greater than
 360 msec in duration.
 The variance was selected to give the error crossover probabilities on
 an element basis to correspond to the good, fair, and poor operator defined
 in section III.B.) The waveform generated by this process is used to modulate
 a carrier of frequency go 4 KHZ, which is simulated by discrete-time process
 sampled at 8 kHz.
 This carrier is then subjected to the fading model (VII.
 C) and white gaussian noise of selectable power is added.
 This received carrier is then input to the receiver, bandpass filter and
 detection routines dis- cussed previously.
 The output of the envelope detector, adjusted in level by subroutine NOISE,
 is then input to the main processing algorithm, PROCESS; the demodulated,
 decoded and translated results are presented on a CRT from which hard copies
 may be obtained.
 
\end_layout

\begin_layout Standard
The overall objective of the simulation experiment is to determine how well
 the finite-path suboptimal estimator performs relative to the optimal estimator.
 Since it is not possible to code the exact optimal estimator due to exponential
ly expanding memory and computation, the lower bounds an error rate derived
 in Section III are used as a basis for comparison.
 Secondly the performance of the tree decoder (the term tree decoder will
 be used to refer to the suboptimal finite-path estimator) relative to other
 simpler techniques is to be evaluated.
 Finally the performance of the tree decoder as a near-optimal demodulator
 for Morse- code is to be obtained and compared to the performance of the
 linear matched filter with integration time equal to the basic element
 duration.
 
\end_layout

\begin_layout Subsection
THE IDEALIZED KAM TREE DECODER 
\end_layout

\begin_layout Standard
The idealization assumptions made in Section III for deriving the lower
 bounds on error rate can be obtained by constraining the estimation algorithm
 to have path branching only at the possible transition times of a synchronous
 KAM signal, and by making the input a true baseband Morse wave- form with
 added white gaussian noise and no fading.
 This experiment was run in order to determine the validity of the lower
 bounds derived there and to obtain a data base for evaluating the sensitivity
 of the tree decoder to non-ideal conditions.
 The results of this experiment are shown in Figure 14 for the three cases
 of first-order and second-order symbols and independent letters.
 Clearly under these ideal conditions the lower bound is very nearly obtainable.
 
\end_layout

\begin_layout Standard
Also shown for comparison are the results of demodulation accomplished by
 linear matched filtering with decoding accomplished by thresholding the
 durations at 2T, where T is the basic element duration.
 These results show that the demodulation provided by the tree decoder is
 clearly superior to the matched filter, and that the independent letter
 model is of sufficient complexity to obtain near-optimal demodulation .
 
\end_layout

\begin_layout Standard
Next, the effect of lack of synchronization was obtained by removing the
 branching constraint on the paths, but still keeping the same idealized
 input signal.
 The results are shown in Figure 15.
 By comparing with the results for the synchronous case, it is obvious that
 at the lower SNR's the performance is degraded.
 
\end_layout

\begin_layout Standard
The next effect to be investigated was the sensitivity to noise statistics
 in the estimator's lack of knowledge of the true noise power.
 These results, shown in Table XIII, indicate that the estimator is relatively
 insensitive to incorrect estimates of noise power within a reasonable range.
 
\end_layout

\begin_layout Standard
Hv.
 
\end_layout

\begin_layout Standard
, 
\end_layout

\begin_layout Standard
TABLE XIII 
\end_layout

\begin_layout Standard
NOISE POWER EST SENSITIVITY (20 wpm KAM) 
\end_layout

\begin_layout Standard
SNR Est Used by Decoder (dB) 9 6 3 2 1 
\end_layout

\begin_layout Standard
TRUE SNR (dB) (100 Hz) 
\end_layout

\begin_layout Standard
Q.
 O 
\end_layout

\begin_layout Standard
LTR Error 
\end_layout

\begin_layout Standard
9 
\end_layout

\begin_layout Standard
- 
\end_layout

\begin_layout Standard
6 
\end_layout

\begin_layout Standard
2 
\end_layout

\begin_layout Standard
1 
\end_layout

\begin_layout Standard
1 
\end_layout

\begin_layout Standard
1 
\end_layout

\begin_layout Standard
3 
\end_layout

\begin_layout Standard
9 
\end_layout

\begin_layout Standard
6 
\end_layout

\begin_layout Standard
5 
\end_layout

\begin_layout Standard
5 
\end_layout

\begin_layout Standard
2 
\end_layout

\begin_layout Standard
— 
\end_layout

\begin_layout Standard
19 
\end_layout

\begin_layout Standard
14 
\end_layout

\begin_layout Standard
14 
\end_layout

\begin_layout Subsection
THE REALISTIC HKM TREE DECODER 
\end_layout

\begin_layout Standard
Although the results discussed above are of theoretical interest since they
 demonstrate a high degree of correla- tion with theory, they have little
 practical value in determining the performance of the demodulator and decoder
 functions under more realistic signal conditions.
 The first series of tests used a KAM signal as input, in order to correspond
 the results to those above for the idealized case and to obtain a basis
 for comparison with the HKM case.
 Table XIV shows the performance of the tree decoder as a function of the
 decoder constraint length (decode delay) and as a function of the degree
 of optimality of the estimator.
 (The degree of optimality is given by the 
\end_layout

\begin_layout Standard
130 
\end_layout

\begin_layout Standard
TABLE XIV 
\end_layout

\begin_layout Standard
Performance of First-Order Markov Decoder vs.
 Decode Delay and Degree Of Estimator Optimality - 50 wpm KAM 
\end_layout

\begin_layout Standard
Decode Delay (Samples) 
\end_layout

\begin_layout Standard
Degree Optimal 
\end_layout

\begin_layout Standard
« P opt J 
\end_layout

\begin_layout Standard
of ity 
\end_layout

\begin_layout Standard
SNR 
\end_layout

\begin_layout Standard
(100 Hz) dB 
\end_layout

\begin_layout Standard
Avg .
 No .
 of Paths Saved 
\end_layout

\begin_layout Standard
parameter P , discussed above, where only enough paths 
\end_layout

\begin_layout Standard
are saved such that the sum of the computed posterior path 
\end_layout

\begin_layout Standard
probabilities > P .
 .) These results show that the 90% r — opt 
\end_layout

\begin_layout Standard
optimal estimator with a decode delay of 200 (1 second) is very nearly as
 good the 98% optimal decoder.
 These values were selected, then, for the remaining tests.
 Table XV shows the performance of the tree decoder as a function of model
 complexity, and the improvement in performance with increasing complexity
 at the lower SNR's is evident.
 For comparison the results for the independent letter model are plotted
 in Figure 16 along with the results for the idealized case, and the lower
 bound for envelope detection.
 
\end_layout

\begin_layout Standard
TABLE XV 
\end_layout

\begin_layout Standard
PERFORMANCE OF DECODER VS.
 MODEL COMPLEXITY - 90% OPTIMAL ESTIMATOR, KAM SIGNAL 
\end_layout

\begin_layout Standard
DECODER MODEL 
\end_layout

\begin_layout Standard
First 
\end_layout

\begin_layout Standard
Second 
\end_layout

\begin_layout Standard
Indep 
\end_layout

\begin_layout Standard
Avg no .
 
\end_layout

\begin_layout Standard
Speed 
\end_layout

\begin_layout Standard
SNR 
\end_layout

\begin_layout Standard
[dB) 
\end_layout

\begin_layout Standard
The next series of tests used a simulated hand-keyed signal as input at
 nominal speeds of 20 and 3 wpm.
 The performance for the good, fair, and poor keying character- istics (element
 error probabilities of .00143, .0149, and .0403 respectively) was evaluated
 for P = .9, and decode delay = 200 as a function of model complexity.
 These results are tabulated in Table XVI.
 The result for the fair sender is shown in Figure 17 along with the corres-
 ponding result for the KAM signal and the theoretical lower bound.
 
\end_layout

\begin_layout Standard
TABLE XVI Decoder Performance For Simulated Hand-Keyed Morse 
\end_layout

\begin_layout Standard
6 
\end_layout

\begin_layout Standard
* Data for MAUDE & Quasi-Bayes From [14, p.
 74] 
\end_layout

\begin_layout Standard
138 
\end_layout

\begin_layout Subsection
STATISTICAL SIGNIFICANCE OF EXPERIMENTAL RESULTS 
\end_layout

\begin_layout Standard
The sample size used in each of the experiments des- cribed was approximately
 200 letters.
 Since the sample size is greater than 30, and since each experiment was
 performed under well-controlled conditions, the outcome of each experiment
 (proportion of letter errors) may be reasonably assumed to be a sample
 point arising from a gaussian density.
 Under this assumption, the following 90% confidence intervals [23] are
 applicable (Table XX) 
\end_layout

\begin_layout Standard
TABLE XX 90%-CONFIDENCE INTERVAL FOR EXPERIMENTAL RESULTS 
\end_layout

\begin_layout Standard
MEASURED EXPERIMENTAL 90% CONFIDENCE 
\end_layout

\begin_layout Standard
ERROR RATE INTERVAL 
\end_layout

\begin_layout Standard
5 % 3 -o — b •s 
\end_layout

\begin_layout Standard
10% 7%-14% 
\end_layout

\begin_layout Standard
15% 11%-19% 
\end_layout

\begin_layout Standard
20% 15%-26% 
\end_layout

\begin_layout Standard
25% 20%-31% 
\end_layout

\begin_layout Standard
30% 24%-36% 
\end_layout

\begin_layout Standard
139 
\end_layout

\begin_layout Standard
While the relatively small sample size of 200 letters is adequate for the
 well-controlled simulation experiments, because of the consistency of the
 input signals, a much larger sample size would be required for testing
 against actual data.
 Because of the lengthy processing time required on the PDP-10 implementation
 (one minute of data requires approximately 20 minutes of processing time)
 , however, it was not feasible to obtain large quantities of test data
 against actual signals.
 The following field results given in Tables XXI and XXII, therefore should
 be considered a proof of feasibility of the tree-decoder, but not necessarily
 typical of results to be expected under a wide range of signal and keying
 characteristics.
 
\end_layout

\begin_layout Standard
140 
\end_layout

\begin_layout Section
PRELIMINARY RESULTS FROM FIELD DATA 
\end_layout

\begin_layout Standard
In order to obtain an estimate of the projected performance of the tree
 decoder under actual signal and channel conditions, the algorithm was tested
 against several tape recordings of signals made in the field.
 Analog tape recordings of the output of a receiver using a 4 kHz IF band
 width with fast-attack, moderate-speed decay (approx.
 200 msec) AGC were made.
 These tapes were digitized using a sample rate of 8 kHz.
 Each cut is approximately 50 seconds in duration, resulting in a relatively
 small, but significant, data base for analysis.
 The text in each case was context-free, and all signals were of sufficiently
 high signal-to-noise ratio so that the true transmitted text could be recovered
 from the detected output.
 The results of these tests are shown in Tables XXI and XXII for the KAM
 and HKM signals respectively.
 
\end_layout

\begin_layout Standard
TABLE XXI 
\end_layout

\begin_layout Standard
PERFORMANCE OF TREE DECODER AGAINST ACTUAL SIGNALS, KAM SENDER 
\end_layout

\begin_layout Standard
Sample 
\end_layout

\begin_layout Standard
Data Rate 
\end_layout

\begin_layout Standard
Avg 
\end_layout

\begin_layout Standard
SNR (dB) 
\end_layout

\begin_layout Standard
Lette 
\end_layout

\begin_layout Standard
(wpm) 
\end_layout

\begin_layout Standard
(100 Hz) 
\end_layout

\begin_layout Standard
(% 
\end_layout

\begin_layout Standard
35 
\end_layout

\begin_layout Standard
20 
\end_layout

\begin_layout Standard
1% 
\end_layout

\begin_layout Standard
30 
\end_layout

\begin_layout Standard
16 
\end_layout

\begin_layout Standard
2% 
\end_layout

\begin_layout Standard
28 
\end_layout

\begin_layout Standard
16 
\end_layout

\begin_layout Standard
1% 
\end_layout

\begin_layout Standard
32 
\end_layout

\begin_layout Standard
18 
\end_layout

\begin_layout Standard
10% 
\end_layout

\begin_layout Standard
30 
\end_layout

\begin_layout Standard
20 
\end_layout

\begin_layout Standard
8% 
\end_layout

\begin_layout Standard
141 
\end_layout

\begin_layout Standard
TABLE XXII 
\end_layout

\begin_layout Standard
PERFORMANCE OF TREE DECODER AGAINST ACTUAL SIGNALS, HKM SENDER 
\end_layout

\begin_layout Standard
Sample 
\end_layout

\begin_layout Standard
1 2 3 
\end_layout

\begin_layout Standard
4 
\end_layout

\begin_layout Standard
Data Rate 
\end_layout

\begin_layout Standard
Avg SNF, 
\end_layout

\begin_layout Standard
.
 (dB) 
\end_layout

\begin_layout Standard
Letter Error 
\end_layout

\begin_layout Standard
(wpm) 
\end_layout

\begin_layout Standard
(100 
\end_layout

\begin_layout Standard
Hz) 
\end_layout

\begin_layout Standard
(%) 
\end_layout

\begin_layout Standard
18 
\end_layout

\begin_layout Standard
20 
\end_layout

\begin_layout Standard
4 
\end_layout

\begin_layout Standard
16 
\end_layout

\begin_layout Standard
16 
\end_layout

\begin_layout Standard
3 
\end_layout

\begin_layout Standard
22 
\end_layout

\begin_layout Standard
18 
\end_layout

\begin_layout Standard
15 
\end_layout

\begin_layout Standard
20 
\end_layout

\begin_layout Standard
20 
\end_layout

\begin_layout Standard
8 
\end_layout

\begin_layout Standard
The disappointing results for samples 4 and 5 of the KAM signals are attributed
 to two effects observed on these cuts.
 Sample 4 contains several long sequences of high- level "static" or "burst"
 noise, which appear in the envelope-detected output as energy which is
 inseparable from true marks of the desired signal.
 Although these false marks are of lower level than the actual signal, the
 algorithm assumes that they are faded marks of the incoming signal and
 demodulates them as such.
 Although the algorithm successfully rejects many of the shorter spurious
 marks because they are inconsistent with the speed of transmission, enough
 are accepted as valid marks to cause the error rate to be high.
 
\end_layout

\begin_layout Standard
In the case of sample 5, all of the errors are attributed to a low level
 Morse interferer which becomes predominant when the desired signal is in
 a word space or pause condition 
\end_layout

\begin_layout Standard
During these times, the receiver gain is not controlled by the relatively
 high-level desired signal, and the under- lying interferer is of sufficient
 SNR (approx.
 8 dB) to be demodulated by the tree decoder algorithm.
 
\end_layout

\begin_layout Standard
For the HKM cuts, the comparatively high error rates for samples 3 and 4
 are attributed to the same type of interf erence/AGC effect discussed above,
 although in sample 3 the interferer is one leg of an FSK teletype signal.
 For all the HKM cuts, the sending quality is rated as good-to-fair 
\end_layout

\begin_layout Section
SUMMARY AND CONCLUSIONS 
\end_layout

\begin_layout Standard
The extinction of communication by Morse telegraphy has been repeatedly
 predicted aperiodically since about 1950.
 While the commercial use of this mode of communica- tions is virtually
 nonexistent in the U.S., except for some maritime services, it is still used
 in the military services of many countries.
 The reliability of Morse links is well-known and long-distance communication,
 particularly at HF, is possible under conditions of interference and atmos-
 pherics which would render other means of communication useless.
 The simplicity, reliability, and efficiency of the receiver (the human
 mind) preclude extinction of this oldest form of successful electrical
 communications.
 
\end_layout

\begin_layout Standard
Radio communication between two persons using Morse code is a distinctly
 human process, involving nuances of code variations and tacitly assumed
 conventions between the communicators, which make machine transcription
 of the human-sent code particularly difficult.
 The theoretical development of a unified structure for modeling a Morse
 message (not just the code itself) presented in this report shows how the
 various aspects of linguistic context, formatting, individualistic operator
 sending peculiarities, and code symbol dependencies may be combined in
 the design of an optimal Morse translator.
 As a practical example of modeling of the Morse message within this structure,
 a model for independent equally-likely letter messages was derived, and
 the resulting decoder was tested against a variety of simulated and actual
 Morse messages.
 
\end_layout

\begin_layout Standard
The results of the simulations show that the error rate of the idealized
 KAM decoder [Fig.
 14,15] approaches the theoretical lower bound for the gaussian channel,
 derived from coding theory arguments, and that the increase in performance
 compared to a linear dot-matched filter can be significant at low signal-to-noi
se ratios.
 Secondly, the performance of the HKM decoder using envelope detection [Fig.
 16] was demonstrated to be only moderately sensitive to the non-gaussian
 nature of the noise statistics at the output of the envelope detector,
 for SNR's above approxi- mately 4 dB in 100 Hz.
 Finally the performance of the HKM tree decoder against simulated hand-keyed
 Morse [Fig.
 17] shows that, under these laboratory conditions, the tree decoder can
 be expected to provide an error rate no worse than that of a human transcriber
 for: (1) output copy with an acceptable error of 10% or less; (2) independent
 equally- likely letter messages.
 In comparison with the MAUDE algorithm, [Table XIX] the tree decoder shows
 a significant decrease in error rate on the simulated data, while in comparison
 with Howe's Quasi-Bayes decoder the error rates are about the same.
 
\end_layout

\begin_layout Standard
These results show that for the case of random letter text, the performance
 of a human operator can be very nearly obtained by optimal non-linear processin
g techniques.
 The estimation algorithm derived in this investigation is adaptive to speed
 changes, varying noise levels and fading signals and has performed for
 approximately 90 hours of running time (approximately 21,000 characters
 total) without exhibiting any noticable signs of divergence or instability.
 The computational burden is severe, however, and for prac- tical use would
 require possibly a pipe-lined approach with digital hardware under microprocess
or control.
 
\end_layout

\begin_layout Standard
The strength of the tree decoder for random letters lies primarily in its
 use of the Morse code structure to perform channel decoding, i.e., demodulation,
 and secon- darily in its use of the structure to accomplish source decoding.
 For contextual messages, however, a well- constructed model of the linguistics,
 semantics, ad format embodied in the structure of an appropriate f, text
 function, describing the evolution of the message states as a finite state
 machine, would add significantly to the error-correction capability of
 the decoder.
 To the extent that such a function can accurately describe the Morse message
 linguistically, the error-rate for contextual messages may be made to approach
 that for the human operator.
 As such, the parallel between the problems of Morse translation and automatic
 speech understanding is evident and therein lies the rub, and perhaps,
 the solution.
 
\end_layout

\begin_layout Section*
APPENDIX SAMPLES OF OUTPUT DATA 
\end_layout

\begin_layout Standard
I.
 In order to obtain an intuitive appeal for the errors produced by the tree
 decoder, several examples of output copy are shown below for various levels
 of keying quality and signal-to-noise ratios.
 Errors are indicated by an underline.
 
\end_layout

\begin_layout Standard
A.
 50 wpm, KAM, 12 dB SNR: 
\end_layout

\begin_layout Standard
A LAZY BROWN DOG JUMPED OVER 2 LOGS ON A SUNNY SUNDAY AFTERNOON 
\end_layout

\begin_layout Standard
B.
 20 wpm, Fair Key, 9 dB SNR: 
\end_layout

\begin_layout Standard
A LAZY BROWN DOG JU^ED OVF 2 LOGS ON I SUNNY SUNDAY AMTERNOON 
\end_layout

\begin_layout Standard
C.
 20 wpm, Fair Key, 6 dB SNR: 
\end_layout

\begin_layout Standard
A LS7 BORWN DOZ JUMPED JHF 2 LOGS ON A SUNNY SUDDAS AFDRNOON 
\end_layout

\begin_layout Standard
D.
 20 wpm, Fair Key, 6 dB SNR (same as C.
 , but with a different noise sequence) : 
\end_layout

\begin_layout Standard
A LSZY BROWN DOZ_ JUMPED OVEL 2 LOGS ON A SUNNY IUTSANO AFTEGNOON 
\end_layout

\begin_layout Standard
147 
\end_layout

\begin_layout Standard
E.
 20 wpm, Fair Key, 4 dB SNR 
\end_layout

\begin_layout Standard
V LAZX HROWN D UD JUMPED JVEL IMI L_OGS ON A SUNNY IM6ACN AFORNOON 
\end_layout

\begin_layout Standard
F.
 15 wpm, KAM, 12 dB SNR 
\end_layout

\begin_layout Standard
CWA6 DE LAB IAW THE QUICK GREY FOX JUMPED OVER THE LAZY BROWN DOG ON A SUNNY
 SUMMER AFTERNOON.
 THIS IS A TEST.
 VW JVXI JGBA GBEY IQNH OPRP CIPU URUC RHIC MUJX SKYQ 
\end_layout

\begin_layout Standard
G.
 15 wpm, Fair Key, 12 dB SNR 
\end_layout

\begin_layout Standard
CWA6 DE HHH IAW THE QUICK GREY FOX JUMPL OVER THE LAZY BROWN NR0GON ASUNNY
 SUMMER AFTERNGON.
 6IS IS A NSCK VW JVXI JGBA GBEY IHIH OPRP CIPU UKUC RMIC MUJX SKYQ 
\end_layout

\begin_layout Standard
H.
 15 wpm, Fair Key, 6 dB SNR 
\end_layout

\begin_layout Standard
C%A6 DE 5HH IAW 5E QUI CO GREY FOX JUMPED OHER T5 LAZY B50W5_ NRO G QN ASUNNY
 SUMMER AFTERNOON 651 S A NSCK VW JVXI JGBA GBE 3SHIH OPRAS CIPU SKUC RHIC
 MUJX SKYQ 
\end_layout

\begin_layout Standard
148 
\end_layout

\begin_layout Standard
II.
 The waveforms shown in the following Figures (Fig.
 
\end_layout

\begin_layout Standard
18) are provided to give a visual appeal to the quality of the signals processed
 by the tree decoder.
 In each figure the input Morse keying signal is on line a.
 Immediately underneath, on line b is the output of the envelope detector
 after the carrier has been modulated by the keying signal, additive noise
 applied, filtered and finally detected.
 On line c is the detected signal, after downsampling to 200 Hz and adjusted
 in level by subroutine NOISE.
 The output of the zero-delay MAP estimate of the keystate (the demodulated
 signal) is on line d.
 These waveforms are the result of processing message E.
 above.
 Note that although the demodulated output in many cases is not correct,
 the correct letter is still decoded, because of the soft decisions utilized
 in the tree-decoder.
 
\end_layout

\begin_layout Section*
LIST OF REFERENCES 
\end_layout

\begin_layout Standard
1.
 Watt, A.D., Coon, R.M.
 , Maxwell, E.L., and Plush, R.W.
 , "Performance of Some Radio Systems in the Presence of Thermal and Atmospheric
 Noise," Proc.
 IRE , Vol 46, 
\end_layout

\begin_layout Standard
Dec 1958.
 
\end_layout

\begin_layout Standard
2.
 Bell, E.L., Processing of the Manual Morse Signal Using Optimal Linear Filtering,
 Smoothing, and Decoding , 
\end_layout

\begin_layout Standard
EE Thesis, Naval Postgraduate School, Monterey, Calif., Sept.
 1975.
 
\end_layout

\begin_layout Standard
3.
 Lane, George, "Signal-to-Noise Requirements for Various Types of Radio
 Telegraphy Service," US Army Communications- Electronics Engineering Installati
on Agency, Electro- magnetics Engineering Division, August 1975.
 
\end_layout

\begin_layout Standard
4.
 Gallager, R.G., Information Theory and Reliable Communication , John Wiley
 and Sons, Inc., New York, 1968.
 
\end_layout

\begin_layout Standard
5.
 Abramson, N.
 , Information Theory and Coding , McGraw Hill, New York, 1963.
 
\end_layout

\begin_layout Standard
6.
 Stein, S.
 and Jones, J., Modern Communication Principles , McGraw-Hill, New York,
 1967.
 
\end_layout

\begin_layout Standard
7.
 Carliolaro, G.
 , and Pierobon, G.
 , "Stationary Symbol Sequences from Variable-Length Word Sequences," IEEE
 Trans.
 Inf.
 Thy , v.
 IT-23, No.
 2, MAR 1977.
 
\end_layout

\begin_layout Standard
8.
 Lee, R.C.K., Optimal Estimation, Identification and Control , The M.I.T.
 Press, Cambridge, Mass.
 1964.
 
\end_layout

\begin_layout Standard
9.
 Sims, F.L.
 and Lainiotis, D.G., "Recursive Algorithm for the Calculation of the Adaptive
 Filter Weighting Coefficients," IEEE Trans.
 Auto.
 Control , vol AC14 , no.
 2, April 1969.
 
\end_layout

\begin_layout Standard
10.
 Wenersson, A., "On Bayesian Estimators for Discrete- Time Linear Systems
 with Markovian Parameters," TRITA-MAT-197 5-5, Dept .
 of Math., Royal Inst, of Technology, Stockholm, Sweden, Jan.
 197 5.
 
\end_layout

\begin_layout Standard
11.
 Yakowitz, S., Williams, T.
 , and Williams, G.
 , "Sur- veillance of Several Markov Targets," IEEE Trans, Inf.
 Thy.
 , vol IT-22, no.
 6, Nov.
 1976.
 
\end_layout

\begin_layout Standard
192 
\end_layout

\begin_layout Standard
12.
 Gold, B.
 , "Machine Recognition of Hand-sent Morse Code," IRE Trans.
 Inf.
 Thy.
 , March 1959.
 
\end_layout

\begin_layout Standard
13.
 Meisel, A., et.
 al., "Morse Laboratory Data Analysis Report," Technology Services Corporation
 Report, May 1976.
 
\end_layout

\begin_layout Standard
14.
 Howe, D.
 , Decision-Directed Modified Quasi-Bayes Estimation and Tracking of the
 Nonstationary Stochastic Parameters of a Communication Signal , Ph.
 D.
 Dissertation , The Catholic University of America, Washington, D.C., 1976.
 
\end_layout

\begin_layout Standard
15.
 Jelinek, F.
 , Bahl, L.
 , and Mercer, R.
 , "Design of a Linguistic Statistical Decoder for the Recognition of Continuous
 Speech," IEEE Trans.
 Inf.
 Thy.
 , Vol IT-21, no.
 3, May 1975.
 
\end_layout

\begin_layout Standard
16.
 Bahl, L.
 and Jelinek, F., "Decoding for Channels with Insertion, Deletions, and Substitut
ions with Applications to Speech Recognition," IEEE Trans.
 Inf.
 Thy.
 , Vol IT-21, no.
 4, July 1975.
 
\end_layout

\begin_layout Standard
17.
 Fung, L.
 , and Fu, K.
 , "Maximum-Likelihood Syntactic Decoding," IEEE Trans.
 Inf.
 Thy.
 , Vol IT-21, no.
 4, July 1975.
 
\end_layout

\begin_layout Standard
18.
 Gelb, A.
 (editor) , Applied Optimal Estimation , The M.I.T.
 Press, Cambridge, Mass., 1974.
 
\end_layout

\begin_layout Standard
19.
 Skolnik, M.
 , Introduction to Radar Systems , McGraw-Hill, New York, 1962.
 
\end_layout

\begin_layout Standard
20.
 Davenport, W.
 , Probability and Random Processes , McGraw-Hill, New York, 1970.
 
\end_layout

\begin_layout Standard
21.
 Schwartz, S.
 , "The Estimator-Correlator for Discrete- time Problems," IEEE Trans.
 Inf.
 Thy.
 , Vol IT-23, 
\end_layout

\begin_layout Standard
no.
 1, Jan 1977.
 
\end_layout

\begin_layout Standard
22.
 Haccoun, D.
 , and Ferguson, M.
 , "Generalized Stack Algorithm for Decoding Convolutional Codes," IEEE
 Trans.
 Inf.
 Thy.
 , Vol IT-21, no.
 6, Nov 1975.
 
\end_layout

\begin_layout Standard
23 .
 Engineering Design Handbook, Experimental Statistics , AMC Pamphlet 706-110,
 Headquarters, U.S.
 Army Materiel Command, Dec 19 69.
 
\end_layout

\begin_layout Standard
193 
\end_layout

\begin_layout Standard
BIBLIOGRAPHY 
\end_layout

\begin_layout Standard
1.
 Bailey, A., and McCann, T.
 , "Application of Printing Telegraph to Long-Wave Radio Circuits," Bell
 System Technical Journal , Vol X, Oct.
 1931.
 
\end_layout

\begin_layout Standard
2.
 Zadeh, L.A.
 , "Optimum Nonlinear Filters," J.
 Appl.
 Physics , Vol 24, no.
 4, April 1953.
 
\end_layout

\begin_layout Standard
3.
 Gonzales, C.
 and Vogler, R.
 , "Automatic Radio Telegraph Translator and Transcriber," Ham Radio , Nov.
 1971.
 
\end_layout

\begin_layout Standard
4.
 Althoff, W.A., An Automatic Radiotelegraph Translator and Transcriber for
 Manually Sent Morse , Master's Thesis, Naval Postgraduate School, Monterey,
 Ca.
 , 
\end_layout

\begin_layout Standard
Dec 1973.
 
\end_layout

\begin_layout Standard
5.
 Forney, G.D., "The Viterbi Algorithm," Proc .
 IEEE , Vol.
 61, no.
 3., March 1973.
 
\end_layout

\begin_layout Standard
6.
 Neuhoff, D.L., "The Viterbi Algorithm as an Aid in Text Recognition," IEEE
 Trans.
 Inf.
 Thy.
 , Vol IT-21, no.
 2, March 1975.
 
\end_layout

\begin_layout Standard
7.
 Manzingo, R.A.
 , "Discrete Optimal Linear Smoothing for Systems with Uncertain Observations,"
 IEEE Trans .
 Inf.
 Thy.
 , Vol IT-21, no.
 3, May 1975.
 
\end_layout

\begin_layout Standard
8.
 Clements, D.
 and Anderson, B.D.O., "A Nonlinear Fixed- Lag Smoother for Finite-State Markov
 Processes," 
\end_layout

\begin_layout Standard
IEEE Trans.
 Inf.
 Thy.
 , Vol IT-21, July 1975.
 
\end_layout

\begin_layout Standard
9.
 Alspach, D.L.
 and Sorenson, H.W., "Nonlinear Bayesian Estimation using Gaussian Sum Approximati
ons," IEEE Trans.
 Auto.
 Control , Vol AC17, no.
 4, August 1972.
 
\end_layout

\begin_layout Standard
10.
 Gray, R.M.
 , "Sliding-Block Source Coding," IEEE Trans.
 Inf.
 Thy.
 , Vol IT-21, no.
 4, July 1975.
 
\end_layout

\begin_layout Standard
11.
 Gray, R.M.
 , "Time-Invariant Trellis Encoding of Ergodic Discrete-Time Sources with
 a Fidelity Criterion," IEEE Trans.
 Inf.
 Thy.
 , Vol IT-23, no.
 1, Jan 1977.
 
\end_layout

\begin_layout Standard
12.
 Shields, P.C.
 and Neuhoff, D.L., "Block and Sliding- Block Source Coding," IEEE Trans.
 Inf.
 Thy.
 , Vol IT-2 3, no.
 2, March 1977.
 
\end_layout

\begin_layout Standard
194 
\end_layout

\begin_layout Standard
13.
 Lainotis, D.G.
 (Editor), Estimation Theory , American Elsevier Publishing Co., New York,
 1974.
 
\end_layout

\begin_layout Standard
14.
 Meditch, J.S., Stochastic Optimal Linear Estimation and Control , McGraw-Hill,
 New York, 196 9.
 
\end_layout

\begin_layout Standard
15.
 Sage, A.
 P.
 and Melsa, J.L., Estimation Theory with Applications to Communications and
 Control , McGraw-Hill, New York, 1971.
 
\end_layout

\begin_layout Standard
16.
 Nahi, N.E., Estimation Theory and Applications , John Wiley & Sons, Inc.,
 New York 1969.
 
\end_layout

\begin_layout Standard
17.
 Jazwinski, A.H., Stochastic Processes and Filtering Theory, Academic Press,
 New York, 1970.
 
\end_layout

\begin_layout Standard
195 
\end_layout

\begin_layout Standard
INITIAL DISTRIBUTION LIST 
\end_layout

\begin_layout Standard
No.
 Copies 
\end_layout

\begin_layout Standard
1.
 Defense Documentation Center 2 Cameron Station 
\end_layout

\begin_layout Standard
Alexandria, Va.
 22314 
\end_layout

\begin_layout Standard
2.
 Library, Code 0212 2 Naval Postgraduate School 
\end_layout

\begin_layout Standard
Monterey, Ca.
 93940 
\end_layout

\begin_layout Standard
3.
 Professor Donald Kirk 2 Department Chairman 
\end_layout

\begin_layout Standard
Department of Electrical Engineering Naval Postgraduate School Monterey,
 Calif.
 93940 
\end_layout

\begin_layout Standard
4.
 Associate Professor Stephen Jauregui, Jr.
 10 Coe 52Ja 
\end_layout

\begin_layout Standard
Department of Electrical Engineering Naval Postgraduate School Monterey,
 Ca.
 939 40 
\end_layout

\begin_layout Standard
5.
 Dr.
 Robert Fossum 1 Dean of Research 
\end_layout

\begin_layout Standard
Naval Postgraduate School Monterey, CA.
 93940 
\end_layout

\begin_layout Standard
6.
 Professor C.
 Comstock, Code 53Zk 1 Department of Mathematics 
\end_layout

\begin_layout Standard
Naval Postgraduate School Monterey, Ca.
 93940 
\end_layout

\begin_layout Standard
7.
 Professor J.
 Ohlson, Code 5201 1 Dept.
 of Elec Engr.
 
\end_layout

\begin_layout Standard
Naval Postgraduate School Monterey, Ca.
 9394 
\end_layout

\begin_layout Standard
8.
 Professor H.
 Titus, Code 52Ts 1 Dept.
 of Elec Engr 
\end_layout

\begin_layout Standard
Naval Postgraduate School Monterey, Ca.
 93 940 
\end_layout

\begin_layout Standard
9.
 Dr.
 J.
 Friedhoffer, Code R6 1 National Security Agency 
\end_layout

\begin_layout Standard
Ft.
 George G.
 Meade, Md.
 29755 
\end_layout

\begin_layout Standard
196 
\end_layout

\begin_layout Standard
10.
 Lt.
 Edison L.
 Bell, Code R6 1 National Security Agency 
\end_layout

\begin_layout Standard
Ft.
 George G.
 Meade, Md.
 20755 
\end_layout

\begin_layout Standard
11.
 Commander, Naval Security Group Command _1 Naval Security Group Headquarters
 
\end_layout

\begin_layout Standard
3801 Nebraska Ave., N.W.
 Washington, D.C.
 20 890 ATTN: LCDR Campbell, G80 
\end_layout

\begin_layout Standard
12.
 Commander, Naval Electronics Systems Command 3 Naval Electronics Systems
 Command Headquarters PME-107 
\end_layout

\begin_layout Standard
Washington, D.C.
 20360 
\end_layout

\begin_layout Standard
ATTN: Mr.
 R.
 Lesage , Mr.
 F..
 Lebert, CAPT.
 H.
 Leavitt, USN 
\end_layout

\begin_layout Standard
13.
 Commander, Naval Electronics Laboratory Center 1 San Diego, California
 92152 
\end_layout

\begin_layout Standard
ATTN: Mr.
 J.
 Griffin 
\end_layout

\begin_layout Standard
14.
 Director, National Security Agency 4 Group R 
\end_layout

\begin_layout Standard
Ft.
 George G.
 Meade, MD 20755 ATTN: Mr.
 H.
 Rosenbloom 
\end_layout

\begin_layout Standard
Mr.
 I.
 McElvy 
\end_layout

\begin_layout Standard
Mr.
 R.
 Ettinger 
\end_layout

\begin_layout Standard
Mr .
 C .
 Wayne 
\end_layout

\begin_layout Standard
15.
 Army Security Agency 1 Unit Hill Farms Station 
\end_layout

\begin_layout Standard
Warenton, Va.
 22186 ATTN: Dr.
 White 
\end_layout

\begin_layout Standard
16.
 TRW, Inc.
 1 Bldg 90 
\end_layout

\begin_layout Standard
1 Space Park 
\end_layout

\begin_layout Standard
Redondo Beach, Ca.
 902 78 
\end_layout

\begin_layout Standard
ATTN: Dr.
 B.
 Whalen 
\end_layout

\begin_layout Standard
17.
 Sylvania, EDL Systems West 1 P.O.
 Box 20 5 
\end_layout

\begin_layout Standard
Mountain View, Ca.
 94040 ATTN: D.
 Jarvis 
\end_layout

\begin_layout Standard
18.
 Pickering Radio Company 1 Professional Plaza 
\end_layout

\begin_layout Standard
Portsmouth, R.I.
 02871 
\end_layout

\begin_layout Standard
197 
\end_layout

\begin_layout Standard
19.
 ESL, Inc.
 495 Java Dr.
 
\end_layout

\begin_layout Standard
Sunnyvale, California 94086 ATTN: W.
 Phillips 
\end_layout

\begin_layout Standard
20.
 Sanders Assoc.
 95 Canal Street 
\end_layout

\begin_layout Standard
Nashua, New Hampshire 306 ATTN: W.
 Zandi 
\end_layout

\begin_layout Standard
198 
\end_layout

\begin_layout Standard
Thesis 
\end_layout

\begin_layout Standard
836125 c.l 
\end_layout

\begin_layout Standard
171670 
\end_layout

\begin_layout Standard
Bell -** ' ^ u 
\end_layout

\begin_layout Standard
Optimal Bayesian es- timation of the state of a probabil istically mapped
 memory-condition- al Markov process with 
\end_layout

\begin_layout Standard
application to manual Morse decoding.
 
\end_layout

\begin_layout Standard
Thesis J (* R70 
\end_layout

\begin_layout Standard
B36125 Bell 
\end_layout

\begin_layout Standard
c.l Optimal Bayesian es- 
\end_layout

\begin_layout Standard
timation of the state of a probabilistically mapped memory-condition- al
 Markov process with application to manual Morse decoding.
 
\end_layout

\begin_layout Standard
thesB36125 
\end_layout

\begin_layout Standard
Optimal Bayesian estimation of the state 
\end_layout

\begin_layout Standard
3 2768 001 03476 2 
\end_layout

\begin_layout Standard
DUDLEY KNOX LIBRARY 
\end_layout

\end_body
\end_document
